{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "003d3879",
   "metadata": {},
   "source": [
    "# Подготовка модели распознавания рукописных букв и цифр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e0d016c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/ilapr/anaconda3/lib/python3.11/site-packages (2.4.1)\n",
      "Requirement already satisfied: torchvision in /home/ilapr/anaconda3/lib/python3.11/site-packages (0.19.1)\n",
      "Requirement already satisfied: filelock in /home/ilapr/anaconda3/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ilapr/anaconda3/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/ilapr/anaconda3/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ilapr/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ilapr/anaconda3/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/ilapr/anaconda3/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ilapr/anaconda3/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ilapr/anaconda3/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ilapr/anaconda3/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ilapr/anaconda3/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ilapr/anaconda3/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ilapr/anaconda3/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ilapr/anaconda3/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ilapr/anaconda3/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ilapr/anaconda3/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/ilapr/anaconda3/lib/python3.11/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ilapr/anaconda3/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/ilapr/anaconda3/lib/python3.11/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ilapr/anaconda3/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.77)\n",
      "Requirement already satisfied: numpy in /home/ilapr/anaconda3/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ilapr/anaconda3/lib/python3.11/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ilapr/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ilapr/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install emnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "8791e9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import PIL\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize, InterpolationMode\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.transforms import ToTensor, Compose, Resize, Normalize\n",
    "from torchvision.datasets import EMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24882ede",
   "metadata": {},
   "source": [
    "## 1. Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17fa0c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 112800\n",
      "Valid: 18800\n",
      "(min, max): (tensor(-1.), tensor(1.))\n",
      "Image shape (1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "transform = Compose([\n",
    "    # Resize((1, 28*28)),\n",
    "    ToTensor(),\n",
    "    Normalize([0.5], [0.5]),\n",
    "])\n",
    "\n",
    "train_dataset = EMNIST('data/', 'balanced', train=True, download=True, transform=transform)\n",
    "val_dataset = EMNIST('data/', 'balanced', train=False, transform=transform)\n",
    "\n",
    "print('Train:', len(train_dataset))\n",
    "print('Valid:', len(val_dataset))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1000, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1000)\n",
    "\n",
    "print('(min, max):', (train_dataset[0][0].min(), train_dataset[0][0].max()))\n",
    "\n",
    "print('Image shape', train_dataset[1][0].numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6510aa9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = train_dataset.classes\n",
    "n_classes = len(classes)\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f0a19eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 <built-in method size of Tensor object at 0x7f26fe945f70>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -0.9922, -0.9451, -0.7490, -0.7176, -0.8353, -0.9765, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -0.9373, -0.7412, -0.1137, -0.0275, -0.3647, -0.8275, -0.9451,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9843,\n",
       "          -0.3961,  0.2392,  0.9059,  0.9529,  0.8196,  0.2471, -0.2549,\n",
       "          -0.7333, -0.8431, -0.9922, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9686,\n",
       "          -0.0275,  0.6706,  0.9922,  0.9922,  0.9922,  0.9373,  0.8353,\n",
       "           0.5843,  0.2078, -0.8196, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9686,\n",
       "          -0.1059,  0.5922,  0.9922,  0.9922,  0.9922,  0.9922,  0.9765,\n",
       "           0.8980,  0.6235, -0.5373, -0.9451, -0.9922, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -0.6471, -0.0431,  0.8039,  0.9216,  0.9765,  0.9922,  0.9922,\n",
       "           0.9922,  0.9529,  0.6235, -0.2863, -0.7176, -0.9686, -0.9843,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -0.8431, -0.4745,  0.3333,  0.6000,  0.8510,  0.9922,  0.9922,\n",
       "           0.9922,  0.9922,  0.9137,  0.2784, -0.2471, -0.7412, -0.8510,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -0.9765, -0.8118, -0.6471,  0.2549,  0.9686,  0.9922,\n",
       "           1.0000,  0.9922,  0.9922,  0.9373,  0.8353,  0.4902, -0.0510,\n",
       "          -0.9373, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -0.9765, -0.4745, -0.0353,  0.7412,  0.9922,  0.9922,\n",
       "           1.0000,  1.0000,  0.9922,  0.9922,  0.9922,  0.9137,  0.5843,\n",
       "          -0.7490, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -0.9922, -0.9216, -0.0353,  0.5294,  0.9373,  0.9922,  0.9922,\n",
       "           1.0000,  1.0000,  1.0000,  0.9922,  0.9922,  0.9059,  0.5686,\n",
       "          -0.7490, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -0.8431, -0.3961,  0.7255,  0.9373,  0.9922,  0.9922,  0.9922,\n",
       "           0.9765,  0.9686,  0.9765,  0.9922,  0.9765,  0.3882, -0.2549,\n",
       "          -0.9451, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -0.6078,  0.0902,  0.9608,  0.9922,  0.9922,  0.9529,  0.8353,\n",
       "           0.3569,  0.1137,  0.3490,  0.6863,  0.6706, -0.1373, -0.7412,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9843,\n",
       "          -0.3569,  0.3490,  0.9765,  0.9922,  0.9922,  0.8353,  0.4667,\n",
       "          -0.3412, -0.5922, -0.3569, -0.0196, -0.0196, -0.5059, -0.8588,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9686,\n",
       "          -0.0196,  0.6863,  0.9922,  0.9922,  0.9765,  0.3569, -0.3412,\n",
       "          -0.9686, -0.9922, -0.9843, -0.9686, -0.9686, -0.9843, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9686,\n",
       "          -0.0039,  0.7020,  0.9922,  0.9922,  0.9216, -0.0980, -0.7333,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9686,\n",
       "          -0.0039,  0.7020,  0.9922,  0.9922,  0.8275, -0.3569, -0.8353,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9686,\n",
       "          -0.0039,  0.7020,  0.9922,  0.9922,  0.7020, -0.6941, -0.9608,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9686,\n",
       "          -0.0039,  0.7020,  0.9922,  0.9922,  0.7020, -0.7098, -0.9686,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9686,\n",
       "          -0.0039,  0.7020,  0.9922,  0.9922,  0.7020, -0.7098, -0.9686,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9686,\n",
       "          -0.0196,  0.6863,  0.9922,  0.9922,  0.7020, -0.7098, -0.9686,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9686,\n",
       "          -0.1059,  0.5922,  0.9922,  0.9922,  0.6863, -0.7098, -0.9686,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -0.6471, -0.0431,  0.7961,  0.8667,  0.2392, -0.8431, -0.9843,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -0.9608, -0.8431, -0.3725, -0.2157, -0.7412, -0.9922, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -0.9843, -0.8431, -0.7882, -0.9451, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, lbl = train_dataset[0]\n",
    "print(lbl, img.size)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7e2b03",
   "metadata": {},
   "source": [
    "## 2. Построение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944755e2",
   "metadata": {},
   "source": [
    "### Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "76bc9580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_f, train_loader, val_loader, n_epoch, val_fre, stop_score=None):\n",
    "    model.train()\n",
    "    for epoch in range(n_epoch):\n",
    "        loss_sum = 0\n",
    "        print(f'Epoch: {epoch}')\n",
    "        for step, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data).squeeze(1)\n",
    "            loss = loss_f(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print(f'Iter: {step} \\tLoss: {loss.item()}')\n",
    "\n",
    "        print(f'Mean Train Loss: {loss_sum / (step + 1):.6f}', end='\\n\\n')\n",
    "\n",
    "        if epoch % val_fre == 0:\n",
    "            score = validate(model, val_loader, loss_f)\n",
    "            if not (score is None):\n",
    "                if score >= stop_score:\n",
    "                    break\n",
    "\n",
    "def validate(model, val_loader, loss_f):\n",
    "    model.eval()\n",
    "    loss_sum = 0\n",
    "    correct = 0\n",
    "    for step, (data, target) in enumerate(val_loader):\n",
    "        with torch.no_grad():\n",
    "            output = model(data).squeeze(1)\n",
    "            loss = loss_f(output, target)\n",
    "        loss_sum += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    acc = correct / len(val_loader.dataset)\n",
    "    print(f'Val Loss: {loss_sum / (step + 1):.6f} \\tAccuracy: {acc}')\n",
    "    model.train()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562aa56a",
   "metadata": {},
   "source": [
    "### Модель 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be800bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hid_features, n_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(in_features, hid_features)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hid_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flat(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a3f6816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=1024, out_features=47, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_mlp = MLP(in_features=28*28, hid_features=1024, n_classes=n_classes)\n",
    "print(model_mlp)\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_mlp.parameters(), lr=1e-1)\n",
    "\n",
    "n_epoch = 20\n",
    "val_fre = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2640b418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Iter: 0 \tLoss: 3.867163896560669\n",
      "Iter: 10 \tLoss: 3.494060754776001\n",
      "Iter: 20 \tLoss: 3.074035406112671\n",
      "Iter: 30 \tLoss: 2.6450343132019043\n",
      "Iter: 40 \tLoss: 2.3304078578948975\n",
      "Iter: 50 \tLoss: 2.014904260635376\n",
      "Iter: 60 \tLoss: 1.8162835836410522\n",
      "Iter: 70 \tLoss: 1.7267307043075562\n",
      "Iter: 80 \tLoss: 1.6570755243301392\n",
      "Iter: 90 \tLoss: 1.557195782661438\n",
      "Iter: 100 \tLoss: 1.5408804416656494\n",
      "Iter: 110 \tLoss: 1.5594550371170044\n",
      "Mean Train Loss: 2.229496\n",
      "\n",
      "Val Loss: 1.497547 \tAccuracy: 0.590531914893617\n",
      "Epoch: 1\n",
      "Iter: 0 \tLoss: 1.413588047027588\n",
      "Iter: 10 \tLoss: 1.469893217086792\n",
      "Iter: 20 \tLoss: 1.4606088399887085\n",
      "Iter: 30 \tLoss: 1.36228609085083\n",
      "Iter: 40 \tLoss: 1.356097936630249\n",
      "Iter: 50 \tLoss: 1.3363268375396729\n",
      "Iter: 60 \tLoss: 1.4339755773544312\n",
      "Iter: 70 \tLoss: 1.3009954690933228\n",
      "Iter: 80 \tLoss: 1.3081750869750977\n",
      "Iter: 90 \tLoss: 1.3987444639205933\n",
      "Iter: 100 \tLoss: 1.341029167175293\n",
      "Iter: 110 \tLoss: 1.2340338230133057\n",
      "Mean Train Loss: 1.365774\n",
      "\n",
      "Epoch: 2\n",
      "Iter: 0 \tLoss: 1.3537696599960327\n",
      "Iter: 10 \tLoss: 1.3663321733474731\n",
      "Iter: 20 \tLoss: 1.311891794204712\n",
      "Iter: 30 \tLoss: 1.396813154220581\n",
      "Iter: 40 \tLoss: 1.144153356552124\n",
      "Iter: 50 \tLoss: 1.2001490592956543\n",
      "Iter: 60 \tLoss: 1.271921992301941\n",
      "Iter: 70 \tLoss: 1.2211297750473022\n",
      "Iter: 80 \tLoss: 1.2023407220840454\n",
      "Iter: 90 \tLoss: 1.1859208345413208\n",
      "Iter: 100 \tLoss: 1.2295140027999878\n",
      "Iter: 110 \tLoss: 1.1684759855270386\n",
      "Mean Train Loss: 1.227190\n",
      "\n",
      "Epoch: 3\n",
      "Iter: 0 \tLoss: 1.2283027172088623\n",
      "Iter: 10 \tLoss: 1.1219784021377563\n",
      "Iter: 20 \tLoss: 1.1981292963027954\n",
      "Iter: 30 \tLoss: 1.1392438411712646\n",
      "Iter: 40 \tLoss: 1.1485689878463745\n",
      "Iter: 50 \tLoss: 1.1140525341033936\n",
      "Iter: 60 \tLoss: 1.1596119403839111\n",
      "Iter: 70 \tLoss: 1.0741733312606812\n",
      "Iter: 80 \tLoss: 1.1247740983963013\n",
      "Iter: 90 \tLoss: 1.1025264263153076\n",
      "Iter: 100 \tLoss: 1.0077589750289917\n",
      "Iter: 110 \tLoss: 1.107403039932251\n",
      "Mean Train Loss: 1.144772\n",
      "\n",
      "Epoch: 4\n",
      "Iter: 0 \tLoss: 1.096737265586853\n",
      "Iter: 10 \tLoss: 1.1304329633712769\n",
      "Iter: 20 \tLoss: 1.0480787754058838\n",
      "Iter: 30 \tLoss: 1.1188867092132568\n",
      "Iter: 40 \tLoss: 1.1452271938323975\n",
      "Iter: 50 \tLoss: 1.0930372476577759\n",
      "Iter: 60 \tLoss: 1.047754168510437\n",
      "Iter: 70 \tLoss: 1.0490909814834595\n",
      "Iter: 80 \tLoss: 1.0419368743896484\n",
      "Iter: 90 \tLoss: 1.0385701656341553\n",
      "Iter: 100 \tLoss: 1.114917516708374\n",
      "Iter: 110 \tLoss: 1.0572295188903809\n",
      "Mean Train Loss: 1.078061\n",
      "\n",
      "Epoch: 5\n",
      "Iter: 0 \tLoss: 1.0900568962097168\n",
      "Iter: 10 \tLoss: 1.0606482028961182\n",
      "Iter: 20 \tLoss: 1.0692743062973022\n",
      "Iter: 30 \tLoss: 1.0858293771743774\n",
      "Iter: 40 \tLoss: 1.042441964149475\n",
      "Iter: 50 \tLoss: 1.0030914545059204\n",
      "Iter: 60 \tLoss: 0.9734253287315369\n",
      "Iter: 70 \tLoss: 1.0364950895309448\n",
      "Iter: 80 \tLoss: 0.9592030644416809\n",
      "Iter: 90 \tLoss: 0.9654414057731628\n",
      "Iter: 100 \tLoss: 0.9821745157241821\n",
      "Iter: 110 \tLoss: 0.9674637317657471\n",
      "Mean Train Loss: 1.017548\n",
      "\n",
      "Epoch: 6\n",
      "Iter: 0 \tLoss: 0.9301751255989075\n",
      "Iter: 10 \tLoss: 0.9397965669631958\n",
      "Iter: 20 \tLoss: 0.9879410862922668\n",
      "Iter: 30 \tLoss: 0.9052677750587463\n",
      "Iter: 40 \tLoss: 0.968162477016449\n",
      "Iter: 50 \tLoss: 0.9288866519927979\n",
      "Iter: 60 \tLoss: 0.8931514620780945\n",
      "Iter: 70 \tLoss: 0.9566476941108704\n",
      "Iter: 80 \tLoss: 0.9297113418579102\n",
      "Iter: 90 \tLoss: 0.9620552659034729\n",
      "Iter: 100 \tLoss: 0.9625334739685059\n",
      "Iter: 110 \tLoss: 0.9288087487220764\n",
      "Mean Train Loss: 0.961006\n",
      "\n",
      "Epoch: 7\n",
      "Iter: 0 \tLoss: 0.9830837249755859\n",
      "Iter: 10 \tLoss: 0.9159892797470093\n",
      "Iter: 20 \tLoss: 0.879878580570221\n",
      "Iter: 30 \tLoss: 0.9001026749610901\n",
      "Iter: 40 \tLoss: 0.8184477686882019\n",
      "Iter: 50 \tLoss: 0.9461753368377686\n",
      "Iter: 60 \tLoss: 0.9505259394645691\n",
      "Iter: 70 \tLoss: 0.937181830406189\n",
      "Iter: 80 \tLoss: 0.9103356003761292\n",
      "Iter: 90 \tLoss: 0.927945613861084\n",
      "Iter: 100 \tLoss: 0.902570903301239\n",
      "Iter: 110 \tLoss: 0.8962953090667725\n",
      "Mean Train Loss: 0.909808\n",
      "\n",
      "Epoch: 8\n",
      "Iter: 0 \tLoss: 0.936087965965271\n",
      "Iter: 10 \tLoss: 0.760846734046936\n",
      "Iter: 20 \tLoss: 0.8757508397102356\n",
      "Iter: 30 \tLoss: 0.857460081577301\n",
      "Iter: 40 \tLoss: 0.8338108062744141\n",
      "Iter: 50 \tLoss: 0.8920971751213074\n",
      "Iter: 60 \tLoss: 0.8588243722915649\n",
      "Iter: 70 \tLoss: 0.8688418865203857\n",
      "Iter: 80 \tLoss: 0.8246785998344421\n",
      "Iter: 90 \tLoss: 0.8394044637680054\n",
      "Iter: 100 \tLoss: 0.8537232875823975\n",
      "Iter: 110 \tLoss: 0.836511492729187\n",
      "Mean Train Loss: 0.862153\n",
      "\n",
      "Epoch: 9\n",
      "Iter: 0 \tLoss: 0.8265669941902161\n",
      "Iter: 10 \tLoss: 0.864825427532196\n",
      "Iter: 20 \tLoss: 0.8214510679244995\n",
      "Iter: 30 \tLoss: 0.8753896951675415\n",
      "Iter: 40 \tLoss: 0.873876690864563\n",
      "Iter: 50 \tLoss: 0.8142905831336975\n",
      "Iter: 60 \tLoss: 0.8692240118980408\n",
      "Iter: 70 \tLoss: 0.8188685178756714\n",
      "Iter: 80 \tLoss: 0.8682934045791626\n",
      "Iter: 90 \tLoss: 0.8273601531982422\n",
      "Iter: 100 \tLoss: 0.801565408706665\n",
      "Iter: 110 \tLoss: 0.7986067533493042\n",
      "Mean Train Loss: 0.819940\n",
      "\n",
      "Epoch: 10\n",
      "Iter: 0 \tLoss: 0.7621701955795288\n",
      "Iter: 10 \tLoss: 0.7715562582015991\n",
      "Iter: 20 \tLoss: 0.7838573455810547\n",
      "Iter: 30 \tLoss: 0.7748152017593384\n",
      "Iter: 40 \tLoss: 0.8265683650970459\n",
      "Iter: 50 \tLoss: 0.7904883623123169\n",
      "Iter: 60 \tLoss: 0.7374300956726074\n",
      "Iter: 70 \tLoss: 0.7606394290924072\n",
      "Iter: 80 \tLoss: 0.76216059923172\n",
      "Iter: 90 \tLoss: 0.7269887924194336\n",
      "Iter: 100 \tLoss: 0.8079590201377869\n",
      "Iter: 110 \tLoss: 0.7224668264389038\n",
      "Mean Train Loss: 0.781199\n",
      "\n",
      "Val Loss: 0.807900 \tAccuracy: 0.7676063829787234\n",
      "Epoch: 11\n",
      "Iter: 0 \tLoss: 0.7798150777816772\n",
      "Iter: 10 \tLoss: 0.7357724905014038\n",
      "Iter: 20 \tLoss: 0.8127703070640564\n",
      "Iter: 30 \tLoss: 0.7780418992042542\n",
      "Iter: 40 \tLoss: 0.7830855250358582\n",
      "Iter: 50 \tLoss: 0.7653968930244446\n",
      "Iter: 60 \tLoss: 0.7544522285461426\n",
      "Iter: 70 \tLoss: 0.7232441306114197\n",
      "Iter: 80 \tLoss: 0.7505910992622375\n",
      "Iter: 90 \tLoss: 0.7488005757331848\n",
      "Iter: 100 \tLoss: 0.7491141557693481\n",
      "Iter: 110 \tLoss: 0.7022252678871155\n",
      "Mean Train Loss: 0.747635\n",
      "\n",
      "Epoch: 12\n",
      "Iter: 0 \tLoss: 0.7333044409751892\n",
      "Iter: 10 \tLoss: 0.7124487161636353\n",
      "Iter: 20 \tLoss: 0.7297990918159485\n",
      "Iter: 30 \tLoss: 0.7004401683807373\n",
      "Iter: 40 \tLoss: 0.6420791149139404\n",
      "Iter: 50 \tLoss: 0.7354970574378967\n",
      "Iter: 60 \tLoss: 0.7479628324508667\n",
      "Iter: 70 \tLoss: 0.7947729825973511\n",
      "Iter: 80 \tLoss: 0.7256085276603699\n",
      "Iter: 90 \tLoss: 0.7273849248886108\n",
      "Iter: 100 \tLoss: 0.6746541261672974\n",
      "Iter: 110 \tLoss: 0.6360430717468262\n",
      "Mean Train Loss: 0.718315\n",
      "\n",
      "Epoch: 13\n",
      "Iter: 0 \tLoss: 0.7124543786048889\n",
      "Iter: 10 \tLoss: 0.6939653754234314\n",
      "Iter: 20 \tLoss: 0.6611661314964294\n",
      "Iter: 30 \tLoss: 0.6502508521080017\n",
      "Iter: 40 \tLoss: 0.7147504687309265\n",
      "Iter: 50 \tLoss: 0.6659439206123352\n",
      "Iter: 60 \tLoss: 0.6934055685997009\n",
      "Iter: 70 \tLoss: 0.7001609206199646\n",
      "Iter: 80 \tLoss: 0.6943942308425903\n",
      "Iter: 90 \tLoss: 0.659127950668335\n",
      "Iter: 100 \tLoss: 0.6706888675689697\n",
      "Iter: 110 \tLoss: 0.695047914981842\n",
      "Mean Train Loss: 0.691936\n",
      "\n",
      "Epoch: 14\n",
      "Iter: 0 \tLoss: 0.6851889491081238\n",
      "Iter: 10 \tLoss: 0.6642873287200928\n",
      "Iter: 20 \tLoss: 0.6423024535179138\n",
      "Iter: 30 \tLoss: 0.7062622904777527\n",
      "Iter: 40 \tLoss: 0.7239115238189697\n",
      "Iter: 50 \tLoss: 0.6361783742904663\n",
      "Iter: 60 \tLoss: 0.7313839793205261\n",
      "Iter: 70 \tLoss: 0.6587016582489014\n",
      "Iter: 80 \tLoss: 0.562160313129425\n",
      "Iter: 90 \tLoss: 0.6715168356895447\n",
      "Iter: 100 \tLoss: 0.657106339931488\n",
      "Iter: 110 \tLoss: 0.6659522652626038\n",
      "Mean Train Loss: 0.667760\n",
      "\n",
      "Epoch: 15\n",
      "Iter: 0 \tLoss: 0.6379472017288208\n",
      "Iter: 10 \tLoss: 0.5932325124740601\n",
      "Iter: 20 \tLoss: 0.6488320827484131\n",
      "Iter: 30 \tLoss: 0.6159150004386902\n",
      "Iter: 40 \tLoss: 0.605845034122467\n",
      "Iter: 50 \tLoss: 0.6442872881889343\n",
      "Iter: 60 \tLoss: 0.6075327396392822\n",
      "Iter: 70 \tLoss: 0.6242024302482605\n",
      "Iter: 80 \tLoss: 0.6529324054718018\n",
      "Iter: 90 \tLoss: 0.6601225137710571\n",
      "Iter: 100 \tLoss: 0.6391043663024902\n",
      "Iter: 110 \tLoss: 0.6821199655532837\n",
      "Mean Train Loss: 0.646595\n",
      "\n",
      "Epoch: 16\n",
      "Iter: 0 \tLoss: 0.6557893753051758\n",
      "Iter: 10 \tLoss: 0.6785105466842651\n",
      "Iter: 20 \tLoss: 0.6412786245346069\n",
      "Iter: 30 \tLoss: 0.7434297800064087\n",
      "Iter: 40 \tLoss: 0.5752286911010742\n",
      "Iter: 50 \tLoss: 0.6610959768295288\n",
      "Iter: 60 \tLoss: 0.5731106996536255\n",
      "Iter: 70 \tLoss: 0.6428805589675903\n",
      "Iter: 80 \tLoss: 0.62494957447052\n",
      "Iter: 90 \tLoss: 0.6553676128387451\n",
      "Iter: 100 \tLoss: 0.5977871417999268\n",
      "Iter: 110 \tLoss: 0.6605823040008545\n",
      "Mean Train Loss: 0.627506\n",
      "\n",
      "Epoch: 17\n",
      "Iter: 0 \tLoss: 0.6043884754180908\n",
      "Iter: 10 \tLoss: 0.6535019874572754\n",
      "Iter: 20 \tLoss: 0.5641800165176392\n",
      "Iter: 30 \tLoss: 0.586938738822937\n",
      "Iter: 40 \tLoss: 0.6328386664390564\n",
      "Iter: 50 \tLoss: 0.5852667093276978\n",
      "Iter: 60 \tLoss: 0.6063711047172546\n",
      "Iter: 70 \tLoss: 0.6046895384788513\n",
      "Iter: 80 \tLoss: 0.6087170243263245\n",
      "Iter: 90 \tLoss: 0.6192120909690857\n",
      "Iter: 100 \tLoss: 0.5842030048370361\n",
      "Iter: 110 \tLoss: 0.6554275751113892\n",
      "Mean Train Loss: 0.610127\n",
      "\n",
      "Epoch: 18\n",
      "Iter: 0 \tLoss: 0.6040566563606262\n",
      "Iter: 10 \tLoss: 0.6077466607093811\n",
      "Iter: 20 \tLoss: 0.5669257640838623\n",
      "Iter: 30 \tLoss: 0.5756773352622986\n",
      "Iter: 40 \tLoss: 0.6333168745040894\n",
      "Iter: 50 \tLoss: 0.5543648600578308\n",
      "Iter: 60 \tLoss: 0.6525306105613708\n",
      "Iter: 70 \tLoss: 0.6171166896820068\n",
      "Iter: 80 \tLoss: 0.638157069683075\n",
      "Iter: 90 \tLoss: 0.5778165459632874\n",
      "Iter: 100 \tLoss: 0.6412480473518372\n",
      "Iter: 110 \tLoss: 0.5903596878051758\n",
      "Mean Train Loss: 0.595222\n",
      "\n",
      "Epoch: 19\n",
      "Iter: 0 \tLoss: 0.5838623642921448\n",
      "Iter: 10 \tLoss: 0.5390340685844421\n",
      "Iter: 20 \tLoss: 0.561417818069458\n",
      "Iter: 30 \tLoss: 0.5403706431388855\n",
      "Iter: 40 \tLoss: 0.5396844148635864\n",
      "Iter: 50 \tLoss: 0.5613368153572083\n",
      "Iter: 60 \tLoss: 0.576696515083313\n",
      "Iter: 70 \tLoss: 0.5860298871994019\n",
      "Iter: 80 \tLoss: 0.6178723573684692\n",
      "Iter: 90 \tLoss: 0.5958314538002014\n",
      "Iter: 100 \tLoss: 0.6312410235404968\n",
      "Iter: 110 \tLoss: 0.6385195851325989\n",
      "Mean Train Loss: 0.580461\n",
      "\n",
      "Val Loss: 0.635273 \tAccuracy: 0.8092553191489362\n"
     ]
    }
   ],
   "source": [
    "train(model_mlp, optimizer, loss_f, train_loader, val_loader, n_epoch, val_fre)\n",
    "validate(model_mlp, val_loader, loss_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ed8132",
   "metadata": {},
   "source": [
    "### Модель 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2df7d217",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hid_features, n_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.c2d = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding='same')\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, hid_features)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(hid_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.c2d(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.flat(x)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e49a96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (c2d): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (relu): ReLU()\n",
      "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
      "  (relu3): ReLU()\n",
      "  (fc2): Linear(in_features=1024, out_features=47, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_mlp = MLP(in_features=28*28, hid_features=1024, n_classes=n_classes)\n",
    "print(model_mlp)\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_mlp.parameters(), lr=1e-1)\n",
    "\n",
    "n_epoch = 20\n",
    "val_fre = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24f37d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Iter: 0 \tLoss: 3.8581442832946777\n",
      "Iter: 10 \tLoss: 3.724702835083008\n",
      "Iter: 20 \tLoss: 3.4315223693847656\n",
      "Iter: 30 \tLoss: 2.7515220642089844\n",
      "Iter: 40 \tLoss: 2.1507158279418945\n",
      "Iter: 50 \tLoss: 2.0570120811462402\n",
      "Iter: 60 \tLoss: 1.8007221221923828\n",
      "Iter: 70 \tLoss: 1.656969428062439\n",
      "Iter: 80 \tLoss: 1.6705248355865479\n",
      "Iter: 90 \tLoss: 1.4827837944030762\n",
      "Iter: 100 \tLoss: 1.4646893739700317\n",
      "Iter: 110 \tLoss: 1.3434853553771973\n",
      "Mean Train Loss: 2.203719\n",
      "\n",
      "Val Loss: 1.306466 \tAccuracy: 0.6388297872340426\n",
      "Epoch: 1\n",
      "Iter: 0 \tLoss: 1.2571821212768555\n",
      "Iter: 10 \tLoss: 1.256620168685913\n",
      "Iter: 20 \tLoss: 1.204832911491394\n",
      "Iter: 30 \tLoss: 1.1461554765701294\n",
      "Iter: 40 \tLoss: 1.1659326553344727\n",
      "Iter: 50 \tLoss: 1.1643372774124146\n",
      "Iter: 60 \tLoss: 1.1841437816619873\n",
      "Iter: 70 \tLoss: 1.0684943199157715\n",
      "Iter: 80 \tLoss: 0.984028160572052\n",
      "Iter: 90 \tLoss: 1.0479137897491455\n",
      "Iter: 100 \tLoss: 0.9675272107124329\n",
      "Iter: 110 \tLoss: 0.9970071911811829\n",
      "Mean Train Loss: 1.126725\n",
      "\n",
      "Epoch: 2\n",
      "Iter: 0 \tLoss: 0.9646202921867371\n",
      "Iter: 10 \tLoss: 0.9881729483604431\n",
      "Iter: 20 \tLoss: 0.9199305176734924\n",
      "Iter: 30 \tLoss: 0.9508216381072998\n",
      "Iter: 40 \tLoss: 0.8937582969665527\n",
      "Iter: 50 \tLoss: 0.8347485065460205\n",
      "Iter: 60 \tLoss: 0.9257687330245972\n",
      "Iter: 70 \tLoss: 0.8726521730422974\n",
      "Iter: 80 \tLoss: 0.8435867428779602\n",
      "Iter: 90 \tLoss: 0.8258731365203857\n",
      "Iter: 100 \tLoss: 0.8617368936538696\n",
      "Iter: 110 \tLoss: 0.7766736745834351\n",
      "Mean Train Loss: 0.869630\n",
      "\n",
      "Epoch: 3\n",
      "Iter: 0 \tLoss: 0.7416834235191345\n",
      "Iter: 10 \tLoss: 0.761949896812439\n",
      "Iter: 20 \tLoss: 0.7837905287742615\n",
      "Iter: 30 \tLoss: 0.7913005948066711\n",
      "Iter: 40 \tLoss: 0.7527987957000732\n",
      "Iter: 50 \tLoss: 0.6694164872169495\n",
      "Iter: 60 \tLoss: 0.6849350929260254\n",
      "Iter: 70 \tLoss: 0.7136124968528748\n",
      "Iter: 80 \tLoss: 0.7162842154502869\n",
      "Iter: 90 \tLoss: 0.688523530960083\n",
      "Iter: 100 \tLoss: 0.7463278770446777\n",
      "Iter: 110 \tLoss: 0.6861774921417236\n",
      "Mean Train Loss: 0.724699\n",
      "\n",
      "Epoch: 4\n",
      "Iter: 0 \tLoss: 0.7017256617546082\n",
      "Iter: 10 \tLoss: 0.6479675769805908\n",
      "Iter: 20 \tLoss: 0.6236793994903564\n",
      "Iter: 30 \tLoss: 0.6342087984085083\n",
      "Iter: 40 \tLoss: 0.6129099726676941\n",
      "Iter: 50 \tLoss: 0.6185656189918518\n",
      "Iter: 60 \tLoss: 0.6156067848205566\n",
      "Iter: 70 \tLoss: 0.6078698635101318\n",
      "Iter: 80 \tLoss: 0.6407783031463623\n",
      "Iter: 90 \tLoss: 0.6851739883422852\n",
      "Iter: 100 \tLoss: 0.5988944172859192\n",
      "Iter: 110 \tLoss: 0.5224995017051697\n",
      "Mean Train Loss: 0.636082\n",
      "\n",
      "Epoch: 5\n",
      "Iter: 0 \tLoss: 0.6357385516166687\n",
      "Iter: 10 \tLoss: 0.6755852699279785\n",
      "Iter: 20 \tLoss: 0.5940014719963074\n",
      "Iter: 30 \tLoss: 0.5922922492027283\n",
      "Iter: 40 \tLoss: 0.53277987241745\n",
      "Iter: 50 \tLoss: 0.5666596293449402\n",
      "Iter: 60 \tLoss: 0.5584158301353455\n",
      "Iter: 70 \tLoss: 0.574981153011322\n",
      "Iter: 80 \tLoss: 0.5341272950172424\n",
      "Iter: 90 \tLoss: 0.5185684561729431\n",
      "Iter: 100 \tLoss: 0.5972375273704529\n",
      "Iter: 110 \tLoss: 0.6131449937820435\n",
      "Mean Train Loss: 0.574460\n",
      "\n",
      "Epoch: 6\n",
      "Iter: 0 \tLoss: 0.5833883285522461\n",
      "Iter: 10 \tLoss: 0.528342068195343\n",
      "Iter: 20 \tLoss: 0.5547449588775635\n",
      "Iter: 30 \tLoss: 0.5541355609893799\n",
      "Iter: 40 \tLoss: 0.5149842500686646\n",
      "Iter: 50 \tLoss: 0.5728344917297363\n",
      "Iter: 60 \tLoss: 0.6188002228736877\n",
      "Iter: 70 \tLoss: 0.49860915541648865\n",
      "Iter: 80 \tLoss: 0.4736800789833069\n",
      "Iter: 90 \tLoss: 0.5330821871757507\n",
      "Iter: 100 \tLoss: 0.5817567110061646\n",
      "Iter: 110 \tLoss: 0.5288435816764832\n",
      "Mean Train Loss: 0.527021\n",
      "\n",
      "Epoch: 7\n",
      "Iter: 0 \tLoss: 0.4531053304672241\n",
      "Iter: 10 \tLoss: 0.4881669580936432\n",
      "Iter: 20 \tLoss: 0.4326653480529785\n",
      "Iter: 30 \tLoss: 0.48455163836479187\n",
      "Iter: 40 \tLoss: 0.5637121200561523\n",
      "Iter: 50 \tLoss: 0.45092707872390747\n",
      "Iter: 60 \tLoss: 0.48667415976524353\n",
      "Iter: 70 \tLoss: 0.47739118337631226\n",
      "Iter: 80 \tLoss: 0.5542991161346436\n",
      "Iter: 90 \tLoss: 0.5347413420677185\n",
      "Iter: 100 \tLoss: 0.47144678235054016\n",
      "Iter: 110 \tLoss: 0.46515923738479614\n",
      "Mean Train Loss: 0.493979\n",
      "\n",
      "Epoch: 8\n",
      "Iter: 0 \tLoss: 0.47928309440612793\n",
      "Iter: 10 \tLoss: 0.5228034853935242\n",
      "Iter: 20 \tLoss: 0.4565111994743347\n",
      "Iter: 30 \tLoss: 0.4376923739910126\n",
      "Iter: 40 \tLoss: 0.4893576502799988\n",
      "Iter: 50 \tLoss: 0.46709975600242615\n",
      "Iter: 60 \tLoss: 0.5011475682258606\n",
      "Iter: 70 \tLoss: 0.45392414927482605\n",
      "Iter: 80 \tLoss: 0.4667920768260956\n",
      "Iter: 90 \tLoss: 0.4531373381614685\n",
      "Iter: 100 \tLoss: 0.4792463481426239\n",
      "Iter: 110 \tLoss: 0.44836148619651794\n",
      "Mean Train Loss: 0.466104\n",
      "\n",
      "Epoch: 9\n",
      "Iter: 0 \tLoss: 0.4661709666252136\n",
      "Iter: 10 \tLoss: 0.45277488231658936\n",
      "Iter: 20 \tLoss: 0.4424186646938324\n",
      "Iter: 30 \tLoss: 0.45500466227531433\n",
      "Iter: 40 \tLoss: 0.4540998935699463\n",
      "Iter: 50 \tLoss: 0.4466313123703003\n",
      "Iter: 60 \tLoss: 0.4382253587245941\n",
      "Iter: 70 \tLoss: 0.3828299641609192\n",
      "Iter: 80 \tLoss: 0.4475804567337036\n",
      "Iter: 90 \tLoss: 0.46964868903160095\n",
      "Iter: 100 \tLoss: 0.4779549241065979\n",
      "Iter: 110 \tLoss: 0.47160258889198303\n",
      "Mean Train Loss: 0.441921\n",
      "\n",
      "Epoch: 10\n",
      "Iter: 0 \tLoss: 0.41699159145355225\n",
      "Iter: 10 \tLoss: 0.3641641139984131\n",
      "Iter: 20 \tLoss: 0.37509685754776\n",
      "Iter: 30 \tLoss: 0.4359355568885803\n",
      "Iter: 40 \tLoss: 0.4724370837211609\n",
      "Iter: 50 \tLoss: 0.4462415874004364\n",
      "Iter: 60 \tLoss: 0.4256095290184021\n",
      "Iter: 70 \tLoss: 0.4676271677017212\n",
      "Iter: 80 \tLoss: 0.4183349907398224\n",
      "Iter: 90 \tLoss: 0.4583032429218292\n",
      "Iter: 100 \tLoss: 0.3877575695514679\n",
      "Iter: 110 \tLoss: 0.43502485752105713\n",
      "Mean Train Loss: 0.419867\n",
      "\n",
      "Val Loss: 0.522396 \tAccuracy: 0.8292553191489361\n",
      "Epoch: 11\n",
      "Iter: 0 \tLoss: 0.3773769438266754\n",
      "Iter: 10 \tLoss: 0.34209758043289185\n",
      "Iter: 20 \tLoss: 0.4340715706348419\n",
      "Iter: 30 \tLoss: 0.4191227853298187\n",
      "Iter: 40 \tLoss: 0.43787112832069397\n",
      "Iter: 50 \tLoss: 0.4253014922142029\n",
      "Iter: 60 \tLoss: 0.4163362383842468\n",
      "Iter: 70 \tLoss: 0.403889536857605\n",
      "Iter: 80 \tLoss: 0.39494794607162476\n",
      "Iter: 90 \tLoss: 0.37097206711769104\n",
      "Iter: 100 \tLoss: 0.397451788187027\n",
      "Iter: 110 \tLoss: 0.39265185594558716\n",
      "Mean Train Loss: 0.403340\n",
      "\n",
      "Epoch: 12\n",
      "Iter: 0 \tLoss: 0.38710904121398926\n",
      "Iter: 10 \tLoss: 0.30630961060523987\n",
      "Iter: 20 \tLoss: 0.36030009388923645\n",
      "Iter: 30 \tLoss: 0.3919878304004669\n",
      "Iter: 40 \tLoss: 0.37829887866973877\n",
      "Iter: 50 \tLoss: 0.4450421631336212\n",
      "Iter: 60 \tLoss: 0.4069586396217346\n",
      "Iter: 70 \tLoss: 0.3864593803882599\n",
      "Iter: 80 \tLoss: 0.3708440959453583\n",
      "Iter: 90 \tLoss: 0.42732423543930054\n",
      "Iter: 100 \tLoss: 0.37272655963897705\n",
      "Iter: 110 \tLoss: 0.3773004114627838\n",
      "Mean Train Loss: 0.386156\n",
      "\n",
      "Epoch: 13\n",
      "Iter: 0 \tLoss: 0.4083908200263977\n",
      "Iter: 10 \tLoss: 0.39191800355911255\n",
      "Iter: 20 \tLoss: 0.4175178110599518\n",
      "Iter: 30 \tLoss: 0.3339078426361084\n",
      "Iter: 40 \tLoss: 0.39374637603759766\n",
      "Iter: 50 \tLoss: 0.3591252267360687\n",
      "Iter: 60 \tLoss: 0.34692785143852234\n",
      "Iter: 70 \tLoss: 0.38607391715049744\n",
      "Iter: 80 \tLoss: 0.3535861372947693\n",
      "Iter: 90 \tLoss: 0.43574684858322144\n",
      "Iter: 100 \tLoss: 0.35998865962028503\n",
      "Iter: 110 \tLoss: 0.34700900316238403\n",
      "Mean Train Loss: 0.372169\n",
      "\n",
      "Epoch: 14\n",
      "Iter: 0 \tLoss: 0.31652164459228516\n",
      "Iter: 10 \tLoss: 0.3547251522541046\n",
      "Iter: 20 \tLoss: 0.30331772565841675\n",
      "Iter: 30 \tLoss: 0.3537377715110779\n",
      "Iter: 40 \tLoss: 0.3446498215198517\n",
      "Iter: 50 \tLoss: 0.3941499590873718\n",
      "Iter: 60 \tLoss: 0.35612624883651733\n",
      "Iter: 70 \tLoss: 0.3775928318500519\n",
      "Iter: 80 \tLoss: 0.3755248486995697\n",
      "Iter: 90 \tLoss: 0.3756369352340698\n",
      "Iter: 100 \tLoss: 0.3809741139411926\n",
      "Iter: 110 \tLoss: 0.35222327709198\n",
      "Mean Train Loss: 0.357648\n",
      "\n",
      "Epoch: 15\n",
      "Iter: 0 \tLoss: 0.34274259209632874\n",
      "Iter: 10 \tLoss: 0.3642694056034088\n",
      "Iter: 20 \tLoss: 0.3072880804538727\n",
      "Iter: 30 \tLoss: 0.3445821702480316\n",
      "Iter: 40 \tLoss: 0.28353333473205566\n",
      "Iter: 50 \tLoss: 0.31863918900489807\n",
      "Iter: 60 \tLoss: 0.3445463478565216\n",
      "Iter: 70 \tLoss: 0.36516982316970825\n",
      "Iter: 80 \tLoss: 0.30036765336990356\n",
      "Iter: 90 \tLoss: 0.35303521156311035\n",
      "Iter: 100 \tLoss: 0.37159955501556396\n",
      "Iter: 110 \tLoss: 0.3421386182308197\n",
      "Mean Train Loss: 0.347402\n",
      "\n",
      "Epoch: 16\n",
      "Iter: 0 \tLoss: 0.30409079790115356\n",
      "Iter: 10 \tLoss: 0.33789145946502686\n",
      "Iter: 20 \tLoss: 0.29393133521080017\n",
      "Iter: 30 \tLoss: 0.33571940660476685\n",
      "Iter: 40 \tLoss: 0.31573981046676636\n",
      "Iter: 50 \tLoss: 0.30244871973991394\n",
      "Iter: 60 \tLoss: 0.3473338484764099\n",
      "Iter: 70 \tLoss: 0.3299424946308136\n",
      "Iter: 80 \tLoss: 0.33064648509025574\n",
      "Iter: 90 \tLoss: 0.32903364300727844\n",
      "Iter: 100 \tLoss: 0.4400649070739746\n",
      "Iter: 110 \tLoss: 0.32026833295822144\n",
      "Mean Train Loss: 0.335359\n",
      "\n",
      "Epoch: 17\n",
      "Iter: 0 \tLoss: 0.331576406955719\n",
      "Iter: 10 \tLoss: 0.32199814915657043\n",
      "Iter: 20 \tLoss: 0.3081669509410858\n",
      "Iter: 30 \tLoss: 0.29582464694976807\n",
      "Iter: 40 \tLoss: 0.3274349868297577\n",
      "Iter: 50 \tLoss: 0.2757259011268616\n",
      "Iter: 60 \tLoss: 0.29111436009407043\n",
      "Iter: 70 \tLoss: 0.3147680163383484\n",
      "Iter: 80 \tLoss: 0.3429624140262604\n",
      "Iter: 90 \tLoss: 0.3224143385887146\n",
      "Iter: 100 \tLoss: 0.3681930899620056\n",
      "Iter: 110 \tLoss: 0.3201840817928314\n",
      "Mean Train Loss: 0.325003\n",
      "\n",
      "Epoch: 18\n",
      "Iter: 0 \tLoss: 0.2971200942993164\n",
      "Iter: 10 \tLoss: 0.3258208632469177\n",
      "Iter: 20 \tLoss: 0.2879500985145569\n",
      "Iter: 30 \tLoss: 0.31460505723953247\n",
      "Iter: 40 \tLoss: 0.31749427318573\n",
      "Iter: 50 \tLoss: 0.27529293298721313\n",
      "Iter: 60 \tLoss: 0.307571679353714\n",
      "Iter: 70 \tLoss: 0.3257676959037781\n",
      "Iter: 80 \tLoss: 0.3048096001148224\n",
      "Iter: 90 \tLoss: 0.31690871715545654\n",
      "Iter: 100 \tLoss: 0.3118879795074463\n",
      "Iter: 110 \tLoss: 0.3284033536911011\n",
      "Mean Train Loss: 0.312968\n",
      "\n",
      "Epoch: 19\n",
      "Iter: 0 \tLoss: 0.3326360285282135\n",
      "Iter: 10 \tLoss: 0.28510141372680664\n",
      "Iter: 20 \tLoss: 0.32449406385421753\n",
      "Iter: 30 \tLoss: 0.2820800244808197\n",
      "Iter: 40 \tLoss: 0.30319926142692566\n",
      "Iter: 50 \tLoss: 0.30390453338623047\n",
      "Iter: 60 \tLoss: 0.33056098222732544\n",
      "Iter: 70 \tLoss: 0.3379833698272705\n",
      "Iter: 80 \tLoss: 0.2878956198692322\n",
      "Iter: 90 \tLoss: 0.29469767212867737\n",
      "Iter: 100 \tLoss: 0.3253815770149231\n",
      "Iter: 110 \tLoss: 0.3208453953266144\n",
      "Mean Train Loss: 0.303917\n",
      "\n",
      "Val Loss: 0.516884 \tAccuracy: 0.8375531914893617\n"
     ]
    }
   ],
   "source": [
    "train(model_mlp, optimizer, loss_f, train_loader, val_loader, n_epoch, val_fre)\n",
    "validate(model_mlp, val_loader, loss_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276e8470",
   "metadata": {},
   "source": [
    "### Модель 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14a85693",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hid_features, n_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.c2d = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding='same')\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        self.c2d_2 = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding='same')\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, hid_features)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(hid_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.c2d(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.c2d_2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.flat(x)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60217138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (c2d): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (relu): ReLU()\n",
      "  (c2d_2): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (relu2): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
      "  (relu3): ReLU()\n",
      "  (fc2): Linear(in_features=1024, out_features=47, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_mlp = MLP(in_features=28*28, hid_features=1024, n_classes=n_classes)\n",
    "print(model_mlp)\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_mlp.parameters(), lr=0.001)\n",
    "\n",
    "n_epoch = 40\n",
    "val_fre = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46082436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Iter: 0 \tLoss: 3.850903272628784\n",
      "Iter: 10 \tLoss: 3.8534061908721924\n",
      "Iter: 20 \tLoss: 3.850444793701172\n",
      "Iter: 30 \tLoss: 3.8454623222351074\n",
      "Iter: 40 \tLoss: 3.822042465209961\n",
      "Iter: 50 \tLoss: 3.6900720596313477\n",
      "Iter: 60 \tLoss: 3.2147529125213623\n",
      "Iter: 70 \tLoss: 2.5563395023345947\n",
      "Iter: 80 \tLoss: 2.2325353622436523\n",
      "Iter: 90 \tLoss: 1.9875737428665161\n",
      "Iter: 100 \tLoss: 1.9590357542037964\n",
      "Iter: 110 \tLoss: 1.724751591682434\n",
      "Mean Train Loss: 3.057067\n",
      "\n",
      "Val Loss: 1.610988 \tAccuracy: 0.5702127659574469\n",
      "Epoch: 1\n",
      "Iter: 0 \tLoss: 1.6707173585891724\n",
      "Iter: 10 \tLoss: 1.72629976272583\n",
      "Iter: 20 \tLoss: 1.5143519639968872\n",
      "Iter: 30 \tLoss: 1.4349641799926758\n",
      "Iter: 40 \tLoss: 1.4561727046966553\n",
      "Iter: 50 \tLoss: 1.3218684196472168\n",
      "Iter: 60 \tLoss: 1.2246131896972656\n",
      "Iter: 70 \tLoss: 1.2752385139465332\n",
      "Iter: 80 \tLoss: 1.1335668563842773\n",
      "Iter: 90 \tLoss: 1.1322309970855713\n",
      "Iter: 100 \tLoss: 1.068444848060608\n",
      "Iter: 110 \tLoss: 1.0698564052581787\n",
      "Mean Train Loss: 1.317193\n",
      "\n",
      "Epoch: 2\n",
      "Iter: 0 \tLoss: 0.9909276366233826\n",
      "Iter: 10 \tLoss: 0.9707860946655273\n",
      "Iter: 20 \tLoss: 0.9616268277168274\n",
      "Iter: 30 \tLoss: 0.9564516544342041\n",
      "Iter: 40 \tLoss: 0.8779544234275818\n",
      "Iter: 50 \tLoss: 0.9258325099945068\n",
      "Iter: 60 \tLoss: 0.8734192848205566\n",
      "Iter: 70 \tLoss: 0.7918950319290161\n",
      "Iter: 80 \tLoss: 0.8856413960456848\n",
      "Iter: 90 \tLoss: 0.7974407076835632\n",
      "Iter: 100 \tLoss: 0.8096473217010498\n",
      "Iter: 110 \tLoss: 0.8383941650390625\n",
      "Mean Train Loss: 0.878873\n",
      "\n",
      "Epoch: 3\n",
      "Iter: 0 \tLoss: 0.7370554208755493\n",
      "Iter: 10 \tLoss: 0.8642750382423401\n",
      "Iter: 20 \tLoss: 0.7870568037033081\n",
      "Iter: 30 \tLoss: 0.7499886751174927\n",
      "Iter: 40 \tLoss: 0.7467581629753113\n",
      "Iter: 50 \tLoss: 0.6552767157554626\n",
      "Iter: 60 \tLoss: 0.6876334547996521\n",
      "Iter: 70 \tLoss: 0.721544086933136\n",
      "Iter: 80 \tLoss: 0.778695821762085\n",
      "Iter: 90 \tLoss: 0.7258071303367615\n",
      "Iter: 100 \tLoss: 0.7139836549758911\n",
      "Iter: 110 \tLoss: 0.6191204786300659\n",
      "Mean Train Loss: 0.719137\n",
      "\n",
      "Epoch: 4\n",
      "Iter: 0 \tLoss: 0.6478079557418823\n",
      "Iter: 10 \tLoss: 0.6308760046958923\n",
      "Iter: 20 \tLoss: 0.6383232474327087\n",
      "Iter: 30 \tLoss: 0.7147855162620544\n",
      "Iter: 40 \tLoss: 0.6341919302940369\n",
      "Iter: 50 \tLoss: 0.6558095216751099\n",
      "Iter: 60 \tLoss: 0.6087905764579773\n",
      "Iter: 70 \tLoss: 0.6141132712364197\n",
      "Iter: 80 \tLoss: 0.5272973775863647\n",
      "Iter: 90 \tLoss: 0.6778106689453125\n",
      "Iter: 100 \tLoss: 0.6366549134254456\n",
      "Iter: 110 \tLoss: 0.6081284284591675\n",
      "Mean Train Loss: 0.639253\n",
      "\n",
      "Epoch: 5\n",
      "Iter: 0 \tLoss: 0.6052790880203247\n",
      "Iter: 10 \tLoss: 0.6682344079017639\n",
      "Iter: 20 \tLoss: 0.6273996829986572\n",
      "Iter: 30 \tLoss: 0.5668615698814392\n",
      "Iter: 40 \tLoss: 0.607739269733429\n",
      "Iter: 50 \tLoss: 0.5937850475311279\n",
      "Iter: 60 \tLoss: 0.5845780372619629\n",
      "Iter: 70 \tLoss: 0.5924372673034668\n",
      "Iter: 80 \tLoss: 0.6043446063995361\n",
      "Iter: 90 \tLoss: 0.631729006767273\n",
      "Iter: 100 \tLoss: 0.6219433546066284\n",
      "Iter: 110 \tLoss: 0.5357889533042908\n",
      "Mean Train Loss: 0.589786\n",
      "\n",
      "Epoch: 6\n",
      "Iter: 0 \tLoss: 0.5366613268852234\n",
      "Iter: 10 \tLoss: 0.5379757285118103\n",
      "Iter: 20 \tLoss: 0.5591634511947632\n",
      "Iter: 30 \tLoss: 0.5202425122261047\n",
      "Iter: 40 \tLoss: 0.6124439239501953\n",
      "Iter: 50 \tLoss: 0.5317623615264893\n",
      "Iter: 60 \tLoss: 0.595097541809082\n",
      "Iter: 70 \tLoss: 0.5612589120864868\n",
      "Iter: 80 \tLoss: 0.4786093831062317\n",
      "Iter: 90 \tLoss: 0.5482019186019897\n",
      "Iter: 100 \tLoss: 0.5930079221725464\n",
      "Iter: 110 \tLoss: 0.547240674495697\n",
      "Mean Train Loss: 0.550211\n",
      "\n",
      "Epoch: 7\n",
      "Iter: 0 \tLoss: 0.6269025206565857\n",
      "Iter: 10 \tLoss: 0.5238620042800903\n",
      "Iter: 20 \tLoss: 0.49765247106552124\n",
      "Iter: 30 \tLoss: 0.5229440927505493\n",
      "Iter: 40 \tLoss: 0.5226714611053467\n",
      "Iter: 50 \tLoss: 0.5628916025161743\n",
      "Iter: 60 \tLoss: 0.5262061357498169\n",
      "Iter: 70 \tLoss: 0.5178670883178711\n",
      "Iter: 80 \tLoss: 0.5111454725265503\n",
      "Iter: 90 \tLoss: 0.6333393454551697\n",
      "Iter: 100 \tLoss: 0.5537022352218628\n",
      "Iter: 110 \tLoss: 0.5531023144721985\n",
      "Mean Train Loss: 0.525508\n",
      "\n",
      "Epoch: 8\n",
      "Iter: 0 \tLoss: 0.4610728621482849\n",
      "Iter: 10 \tLoss: 0.47237467765808105\n",
      "Iter: 20 \tLoss: 0.5109694600105286\n",
      "Iter: 30 \tLoss: 0.46956589818000793\n",
      "Iter: 40 \tLoss: 0.49612361192703247\n",
      "Iter: 50 \tLoss: 0.5363562703132629\n",
      "Iter: 60 \tLoss: 0.542087972164154\n",
      "Iter: 70 \tLoss: 0.4555252492427826\n",
      "Iter: 80 \tLoss: 0.5194498300552368\n",
      "Iter: 90 \tLoss: 0.4891296625137329\n",
      "Iter: 100 \tLoss: 0.5353732109069824\n",
      "Iter: 110 \tLoss: 0.5322046279907227\n",
      "Mean Train Loss: 0.504059\n",
      "\n",
      "Epoch: 9\n",
      "Iter: 0 \tLoss: 0.5097450017929077\n",
      "Iter: 10 \tLoss: 0.48455217480659485\n",
      "Iter: 20 \tLoss: 0.4684569239616394\n",
      "Iter: 30 \tLoss: 0.46182218194007874\n",
      "Iter: 40 \tLoss: 0.49826958775520325\n",
      "Iter: 50 \tLoss: 0.47009748220443726\n",
      "Iter: 60 \tLoss: 0.5038230419158936\n",
      "Iter: 70 \tLoss: 0.4907872974872589\n",
      "Iter: 80 \tLoss: 0.5530855059623718\n",
      "Iter: 90 \tLoss: 0.4928966164588928\n",
      "Iter: 100 \tLoss: 0.49231934547424316\n",
      "Iter: 110 \tLoss: 0.4972556531429291\n",
      "Mean Train Loss: 0.485665\n",
      "\n",
      "Epoch: 10\n",
      "Iter: 0 \tLoss: 0.46362313628196716\n",
      "Iter: 10 \tLoss: 0.4289243221282959\n",
      "Iter: 20 \tLoss: 0.43966954946517944\n",
      "Iter: 30 \tLoss: 0.418693482875824\n",
      "Iter: 40 \tLoss: 0.5217329263687134\n",
      "Iter: 50 \tLoss: 0.5270434617996216\n",
      "Iter: 60 \tLoss: 0.44431206583976746\n",
      "Iter: 70 \tLoss: 0.49352315068244934\n",
      "Iter: 80 \tLoss: 0.49955207109451294\n",
      "Iter: 90 \tLoss: 0.49082204699516296\n",
      "Iter: 100 \tLoss: 0.492862731218338\n",
      "Iter: 110 \tLoss: 0.49429553747177124\n",
      "Mean Train Loss: 0.474991\n",
      "\n",
      "Val Loss: 0.462758 \tAccuracy: 0.8467021276595744\n",
      "Epoch: 11\n",
      "Iter: 0 \tLoss: 0.4379997253417969\n",
      "Iter: 10 \tLoss: 0.4905049800872803\n",
      "Iter: 20 \tLoss: 0.45327067375183105\n",
      "Iter: 30 \tLoss: 0.3830238878726959\n",
      "Iter: 40 \tLoss: 0.4277956485748291\n",
      "Iter: 50 \tLoss: 0.4886864423751831\n",
      "Iter: 60 \tLoss: 0.4523082971572876\n",
      "Iter: 70 \tLoss: 0.456582635641098\n",
      "Iter: 80 \tLoss: 0.3995702564716339\n",
      "Iter: 90 \tLoss: 0.461316853761673\n",
      "Iter: 100 \tLoss: 0.49933451414108276\n",
      "Iter: 110 \tLoss: 0.46606385707855225\n",
      "Mean Train Loss: 0.456676\n",
      "\n",
      "Epoch: 12\n",
      "Iter: 0 \tLoss: 0.41354480385780334\n",
      "Iter: 10 \tLoss: 0.4704626500606537\n",
      "Iter: 20 \tLoss: 0.47504737973213196\n",
      "Iter: 30 \tLoss: 0.4632103145122528\n",
      "Iter: 40 \tLoss: 0.4134620726108551\n",
      "Iter: 50 \tLoss: 0.4568955600261688\n",
      "Iter: 60 \tLoss: 0.4452100396156311\n",
      "Iter: 70 \tLoss: 0.48011887073516846\n",
      "Iter: 80 \tLoss: 0.4646543860435486\n",
      "Iter: 90 \tLoss: 0.44187790155410767\n",
      "Iter: 100 \tLoss: 0.4538247287273407\n",
      "Iter: 110 \tLoss: 0.44629064202308655\n",
      "Mean Train Loss: 0.448174\n",
      "\n",
      "Epoch: 13\n",
      "Iter: 0 \tLoss: 0.4261917173862457\n",
      "Iter: 10 \tLoss: 0.46698954701423645\n",
      "Iter: 20 \tLoss: 0.4004369080066681\n",
      "Iter: 30 \tLoss: 0.45312315225601196\n",
      "Iter: 40 \tLoss: 0.4783891439437866\n",
      "Iter: 50 \tLoss: 0.42862749099731445\n",
      "Iter: 60 \tLoss: 0.4404885768890381\n",
      "Iter: 70 \tLoss: 0.40598952770233154\n",
      "Iter: 80 \tLoss: 0.4417283833026886\n",
      "Iter: 90 \tLoss: 0.42573463916778564\n",
      "Iter: 100 \tLoss: 0.4986836910247803\n",
      "Iter: 110 \tLoss: 0.43270155787467957\n",
      "Mean Train Loss: 0.437149\n",
      "\n",
      "Epoch: 14\n",
      "Iter: 0 \tLoss: 0.44183340668678284\n",
      "Iter: 10 \tLoss: 0.4270170032978058\n",
      "Iter: 20 \tLoss: 0.43039634823799133\n",
      "Iter: 30 \tLoss: 0.42587584257125854\n",
      "Iter: 40 \tLoss: 0.4615667462348938\n",
      "Iter: 50 \tLoss: 0.4024469554424286\n",
      "Iter: 60 \tLoss: 0.4479243755340576\n",
      "Iter: 70 \tLoss: 0.3939947187900543\n",
      "Iter: 80 \tLoss: 0.4114195704460144\n",
      "Iter: 90 \tLoss: 0.42127203941345215\n",
      "Iter: 100 \tLoss: 0.44660884141921997\n",
      "Iter: 110 \tLoss: 0.46619492769241333\n",
      "Mean Train Loss: 0.425319\n",
      "\n",
      "Epoch: 15\n",
      "Iter: 0 \tLoss: 0.397184818983078\n",
      "Iter: 10 \tLoss: 0.42093753814697266\n",
      "Iter: 20 \tLoss: 0.40791556239128113\n",
      "Iter: 30 \tLoss: 0.367157518863678\n",
      "Iter: 40 \tLoss: 0.37716978788375854\n",
      "Iter: 50 \tLoss: 0.3677521049976349\n",
      "Iter: 60 \tLoss: 0.47217682003974915\n",
      "Iter: 70 \tLoss: 0.3701636493206024\n",
      "Iter: 80 \tLoss: 0.49327778816223145\n",
      "Iter: 90 \tLoss: 0.3818680942058563\n",
      "Iter: 100 \tLoss: 0.42694932222366333\n",
      "Iter: 110 \tLoss: 0.38744625449180603\n",
      "Mean Train Loss: 0.414991\n",
      "\n",
      "Epoch: 16\n",
      "Iter: 0 \tLoss: 0.3846259117126465\n",
      "Iter: 10 \tLoss: 0.36359915137290955\n",
      "Iter: 20 \tLoss: 0.40851232409477234\n",
      "Iter: 30 \tLoss: 0.3881695866584778\n",
      "Iter: 40 \tLoss: 0.37813830375671387\n",
      "Iter: 50 \tLoss: 0.4298647940158844\n",
      "Iter: 60 \tLoss: 0.43272149562835693\n",
      "Iter: 70 \tLoss: 0.4024212658405304\n",
      "Iter: 80 \tLoss: 0.3596007525920868\n",
      "Iter: 90 \tLoss: 0.41796964406967163\n",
      "Iter: 100 \tLoss: 0.3404715359210968\n",
      "Iter: 110 \tLoss: 0.40930014848709106\n",
      "Mean Train Loss: 0.408060\n",
      "\n",
      "Epoch: 17\n",
      "Iter: 0 \tLoss: 0.44652485847473145\n",
      "Iter: 10 \tLoss: 0.37077760696411133\n",
      "Iter: 20 \tLoss: 0.39145204424858093\n",
      "Iter: 30 \tLoss: 0.3973250687122345\n",
      "Iter: 40 \tLoss: 0.3695695102214813\n",
      "Iter: 50 \tLoss: 0.4124968647956848\n",
      "Iter: 60 \tLoss: 0.3683643937110901\n",
      "Iter: 70 \tLoss: 0.42518240213394165\n",
      "Iter: 80 \tLoss: 0.35900965332984924\n",
      "Iter: 90 \tLoss: 0.41321325302124023\n",
      "Iter: 100 \tLoss: 0.40210214257240295\n",
      "Iter: 110 \tLoss: 0.422661691904068\n",
      "Mean Train Loss: 0.402368\n",
      "\n",
      "Epoch: 18\n",
      "Iter: 0 \tLoss: 0.40989190340042114\n",
      "Iter: 10 \tLoss: 0.39741483330726624\n",
      "Iter: 20 \tLoss: 0.33194082975387573\n",
      "Iter: 30 \tLoss: 0.41855818033218384\n",
      "Iter: 40 \tLoss: 0.38406985998153687\n",
      "Iter: 50 \tLoss: 0.41424500942230225\n",
      "Iter: 60 \tLoss: 0.4155559241771698\n",
      "Iter: 70 \tLoss: 0.36375415325164795\n",
      "Iter: 80 \tLoss: 0.40504181385040283\n",
      "Iter: 90 \tLoss: 0.41123780608177185\n",
      "Iter: 100 \tLoss: 0.4155103862285614\n",
      "Iter: 110 \tLoss: 0.41925638914108276\n",
      "Mean Train Loss: 0.394047\n",
      "\n",
      "Epoch: 19\n",
      "Iter: 0 \tLoss: 0.41069766879081726\n",
      "Iter: 10 \tLoss: 0.3527466058731079\n",
      "Iter: 20 \tLoss: 0.40912917256355286\n",
      "Iter: 30 \tLoss: 0.3492268919944763\n",
      "Iter: 40 \tLoss: 0.36794859170913696\n",
      "Iter: 50 \tLoss: 0.36841070652008057\n",
      "Iter: 60 \tLoss: 0.3765721321105957\n",
      "Iter: 70 \tLoss: 0.3597770631313324\n",
      "Iter: 80 \tLoss: 0.42098328471183777\n",
      "Iter: 90 \tLoss: 0.38469964265823364\n",
      "Iter: 100 \tLoss: 0.34917521476745605\n",
      "Iter: 110 \tLoss: 0.40503978729248047\n",
      "Mean Train Loss: 0.388555\n",
      "\n",
      "Epoch: 20\n",
      "Iter: 0 \tLoss: 0.3693484663963318\n",
      "Iter: 10 \tLoss: 0.3713560104370117\n",
      "Iter: 20 \tLoss: 0.4257136285305023\n",
      "Iter: 30 \tLoss: 0.41069045662879944\n",
      "Iter: 40 \tLoss: 0.4502025246620178\n",
      "Iter: 50 \tLoss: 0.3590563237667084\n",
      "Iter: 60 \tLoss: 0.368933767080307\n",
      "Iter: 70 \tLoss: 0.3951367437839508\n",
      "Iter: 80 \tLoss: 0.4100976288318634\n",
      "Iter: 90 \tLoss: 0.3560427725315094\n",
      "Iter: 100 \tLoss: 0.34911811351776123\n",
      "Iter: 110 \tLoss: 0.3766013979911804\n",
      "Mean Train Loss: 0.382708\n",
      "\n",
      "Val Loss: 0.438164 \tAccuracy: 0.8569148936170212\n",
      "Epoch: 21\n",
      "Iter: 0 \tLoss: 0.3715572655200958\n",
      "Iter: 10 \tLoss: 0.4025236964225769\n",
      "Iter: 20 \tLoss: 0.3379324972629547\n",
      "Iter: 30 \tLoss: 0.40816062688827515\n",
      "Iter: 40 \tLoss: 0.3864405155181885\n",
      "Iter: 50 \tLoss: 0.3489603102207184\n",
      "Iter: 60 \tLoss: 0.3924071490764618\n",
      "Iter: 70 \tLoss: 0.3808097243309021\n",
      "Iter: 80 \tLoss: 0.3645951747894287\n",
      "Iter: 90 \tLoss: 0.36491283774375916\n",
      "Iter: 100 \tLoss: 0.39886772632598877\n",
      "Iter: 110 \tLoss: 0.38921767473220825\n",
      "Mean Train Loss: 0.374870\n",
      "\n",
      "Epoch: 22\n",
      "Iter: 0 \tLoss: 0.3405744433403015\n",
      "Iter: 10 \tLoss: 0.34627845883369446\n",
      "Iter: 20 \tLoss: 0.34861427545547485\n",
      "Iter: 30 \tLoss: 0.336548775434494\n",
      "Iter: 40 \tLoss: 0.3920999765396118\n",
      "Iter: 50 \tLoss: 0.3630092144012451\n",
      "Iter: 60 \tLoss: 0.3980430066585541\n",
      "Iter: 70 \tLoss: 0.3709297180175781\n",
      "Iter: 80 \tLoss: 0.36148715019226074\n",
      "Iter: 90 \tLoss: 0.4162355661392212\n",
      "Iter: 100 \tLoss: 0.4026916027069092\n",
      "Iter: 110 \tLoss: 0.37225282192230225\n",
      "Mean Train Loss: 0.369927\n",
      "\n",
      "Epoch: 23\n",
      "Iter: 0 \tLoss: 0.414927214384079\n",
      "Iter: 10 \tLoss: 0.35501888394355774\n",
      "Iter: 20 \tLoss: 0.38163208961486816\n",
      "Iter: 30 \tLoss: 0.36811915040016174\n",
      "Iter: 40 \tLoss: 0.3695088326931\n",
      "Iter: 50 \tLoss: 0.350381076335907\n",
      "Iter: 60 \tLoss: 0.34840133786201477\n",
      "Iter: 70 \tLoss: 0.3685077130794525\n",
      "Iter: 80 \tLoss: 0.3972482979297638\n",
      "Iter: 90 \tLoss: 0.31487730145454407\n",
      "Iter: 100 \tLoss: 0.41811633110046387\n",
      "Iter: 110 \tLoss: 0.37525415420532227\n",
      "Mean Train Loss: 0.365214\n",
      "\n",
      "Epoch: 24\n",
      "Iter: 0 \tLoss: 0.3548049330711365\n",
      "Iter: 10 \tLoss: 0.35711669921875\n",
      "Iter: 20 \tLoss: 0.37346649169921875\n",
      "Iter: 30 \tLoss: 0.36641383171081543\n",
      "Iter: 40 \tLoss: 0.3394956588745117\n",
      "Iter: 50 \tLoss: 0.41328951716423035\n",
      "Iter: 60 \tLoss: 0.3543799817562103\n",
      "Iter: 70 \tLoss: 0.34280961751937866\n",
      "Iter: 80 \tLoss: 0.40178439021110535\n",
      "Iter: 90 \tLoss: 0.3336554169654846\n",
      "Iter: 100 \tLoss: 0.3484075367450714\n",
      "Iter: 110 \tLoss: 0.42190301418304443\n",
      "Mean Train Loss: 0.358971\n",
      "\n",
      "Epoch: 25\n",
      "Iter: 0 \tLoss: 0.3503108620643616\n",
      "Iter: 10 \tLoss: 0.3147328794002533\n",
      "Iter: 20 \tLoss: 0.39046114683151245\n",
      "Iter: 30 \tLoss: 0.273764431476593\n",
      "Iter: 40 \tLoss: 0.33606553077697754\n",
      "Iter: 50 \tLoss: 0.3134055435657501\n",
      "Iter: 60 \tLoss: 0.3518235683441162\n",
      "Iter: 70 \tLoss: 0.3370700478553772\n",
      "Iter: 80 \tLoss: 0.3632192313671112\n",
      "Iter: 90 \tLoss: 0.37003612518310547\n",
      "Iter: 100 \tLoss: 0.36827293038368225\n",
      "Iter: 110 \tLoss: 0.3511306643486023\n",
      "Mean Train Loss: 0.356132\n",
      "\n",
      "Epoch: 26\n",
      "Iter: 0 \tLoss: 0.36577674746513367\n",
      "Iter: 10 \tLoss: 0.31215202808380127\n",
      "Iter: 20 \tLoss: 0.31079453229904175\n",
      "Iter: 30 \tLoss: 0.31484830379486084\n",
      "Iter: 40 \tLoss: 0.38137418031692505\n",
      "Iter: 50 \tLoss: 0.32100430130958557\n",
      "Iter: 60 \tLoss: 0.38198763132095337\n",
      "Iter: 70 \tLoss: 0.3425765037536621\n",
      "Iter: 80 \tLoss: 0.34931841492652893\n",
      "Iter: 90 \tLoss: 0.3461942672729492\n",
      "Iter: 100 \tLoss: 0.3423517346382141\n",
      "Iter: 110 \tLoss: 0.3063504993915558\n",
      "Mean Train Loss: 0.348551\n",
      "\n",
      "Epoch: 27\n",
      "Iter: 0 \tLoss: 0.32184478640556335\n",
      "Iter: 10 \tLoss: 0.3152526021003723\n",
      "Iter: 20 \tLoss: 0.36585715413093567\n",
      "Iter: 30 \tLoss: 0.39088496565818787\n",
      "Iter: 40 \tLoss: 0.3626485764980316\n",
      "Iter: 50 \tLoss: 0.32069674134254456\n",
      "Iter: 60 \tLoss: 0.3337794244289398\n",
      "Iter: 70 \tLoss: 0.3424049913883209\n",
      "Iter: 80 \tLoss: 0.32859930396080017\n",
      "Iter: 90 \tLoss: 0.32960033416748047\n",
      "Iter: 100 \tLoss: 0.3684292137622833\n",
      "Iter: 110 \tLoss: 0.3225141167640686\n",
      "Mean Train Loss: 0.347394\n",
      "\n",
      "Epoch: 28\n",
      "Iter: 0 \tLoss: 0.3267671763896942\n",
      "Iter: 10 \tLoss: 0.33815687894821167\n",
      "Iter: 20 \tLoss: 0.33996596932411194\n",
      "Iter: 30 \tLoss: 0.3508131504058838\n",
      "Iter: 40 \tLoss: 0.3399057686328888\n",
      "Iter: 50 \tLoss: 0.3419872224330902\n",
      "Iter: 60 \tLoss: 0.30791589617729187\n",
      "Iter: 70 \tLoss: 0.3660486042499542\n",
      "Iter: 80 \tLoss: 0.35342133045196533\n",
      "Iter: 90 \tLoss: 0.34360748529434204\n",
      "Iter: 100 \tLoss: 0.3131946921348572\n",
      "Iter: 110 \tLoss: 0.3114105761051178\n",
      "Mean Train Loss: 0.343309\n",
      "\n",
      "Epoch: 29\n",
      "Iter: 0 \tLoss: 0.31108981370925903\n",
      "Iter: 10 \tLoss: 0.3174241781234741\n",
      "Iter: 20 \tLoss: 0.34122687578201294\n",
      "Iter: 30 \tLoss: 0.3381534814834595\n",
      "Iter: 40 \tLoss: 0.3352760970592499\n",
      "Iter: 50 \tLoss: 0.2918170988559723\n",
      "Iter: 60 \tLoss: 0.3098563849925995\n",
      "Iter: 70 \tLoss: 0.3369751572608948\n",
      "Iter: 80 \tLoss: 0.36554402112960815\n",
      "Iter: 90 \tLoss: 0.34288328886032104\n",
      "Iter: 100 \tLoss: 0.3614489734172821\n",
      "Iter: 110 \tLoss: 0.3330005705356598\n",
      "Mean Train Loss: 0.336252\n",
      "\n",
      "Epoch: 30\n",
      "Iter: 0 \tLoss: 0.3353917598724365\n",
      "Iter: 10 \tLoss: 0.30638450384140015\n",
      "Iter: 20 \tLoss: 0.3222791254520416\n",
      "Iter: 30 \tLoss: 0.2950983941555023\n",
      "Iter: 40 \tLoss: 0.2833036482334137\n",
      "Iter: 50 \tLoss: 0.30694907903671265\n",
      "Iter: 60 \tLoss: 0.28013789653778076\n",
      "Iter: 70 \tLoss: 0.32933539152145386\n",
      "Iter: 80 \tLoss: 0.3212145268917084\n",
      "Iter: 90 \tLoss: 0.3410272002220154\n",
      "Iter: 100 \tLoss: 0.36126336455345154\n",
      "Iter: 110 \tLoss: 0.3520883321762085\n",
      "Mean Train Loss: 0.332279\n",
      "\n",
      "Val Loss: 0.435964 \tAccuracy: 0.8603723404255319\n",
      "Epoch: 31\n",
      "Iter: 0 \tLoss: 0.277704656124115\n",
      "Iter: 10 \tLoss: 0.328331857919693\n",
      "Iter: 20 \tLoss: 0.30413052439689636\n",
      "Iter: 30 \tLoss: 0.31477394700050354\n",
      "Iter: 40 \tLoss: 0.30101504921913147\n",
      "Iter: 50 \tLoss: 0.30905604362487793\n",
      "Iter: 60 \tLoss: 0.3944407105445862\n",
      "Iter: 70 \tLoss: 0.32815778255462646\n",
      "Iter: 80 \tLoss: 0.3615955710411072\n",
      "Iter: 90 \tLoss: 0.3300987482070923\n",
      "Iter: 100 \tLoss: 0.3391132056713104\n",
      "Iter: 110 \tLoss: 0.3759351670742035\n",
      "Mean Train Loss: 0.328225\n",
      "\n",
      "Epoch: 32\n",
      "Iter: 0 \tLoss: 0.3103055953979492\n",
      "Iter: 10 \tLoss: 0.29955580830574036\n",
      "Iter: 20 \tLoss: 0.3041647970676422\n",
      "Iter: 30 \tLoss: 0.321755051612854\n",
      "Iter: 40 \tLoss: 0.34132784605026245\n",
      "Iter: 50 \tLoss: 0.35900285840034485\n",
      "Iter: 60 \tLoss: 0.32221147418022156\n",
      "Iter: 70 \tLoss: 0.35839495062828064\n",
      "Iter: 80 \tLoss: 0.3134867548942566\n",
      "Iter: 90 \tLoss: 0.33339622616767883\n",
      "Iter: 100 \tLoss: 0.35845082998275757\n",
      "Iter: 110 \tLoss: 0.2886204719543457\n",
      "Mean Train Loss: 0.323778\n",
      "\n",
      "Epoch: 33\n",
      "Iter: 0 \tLoss: 0.30390194058418274\n",
      "Iter: 10 \tLoss: 0.28464558720588684\n",
      "Iter: 20 \tLoss: 0.29250073432922363\n",
      "Iter: 30 \tLoss: 0.35207295417785645\n",
      "Iter: 40 \tLoss: 0.3247421681880951\n",
      "Iter: 50 \tLoss: 0.3147384822368622\n",
      "Iter: 60 \tLoss: 0.2980515956878662\n",
      "Iter: 70 \tLoss: 0.28756093978881836\n",
      "Iter: 80 \tLoss: 0.30625754594802856\n",
      "Iter: 90 \tLoss: 0.3340469002723694\n",
      "Iter: 100 \tLoss: 0.31519994139671326\n",
      "Iter: 110 \tLoss: 0.35427403450012207\n",
      "Mean Train Loss: 0.323545\n",
      "\n",
      "Epoch: 34\n",
      "Iter: 0 \tLoss: 0.3507605195045471\n",
      "Iter: 10 \tLoss: 0.3026648759841919\n",
      "Iter: 20 \tLoss: 0.3364105522632599\n",
      "Iter: 30 \tLoss: 0.3130803406238556\n",
      "Iter: 40 \tLoss: 0.34788620471954346\n",
      "Iter: 50 \tLoss: 0.3320840895175934\n",
      "Iter: 60 \tLoss: 0.3261485993862152\n",
      "Iter: 70 \tLoss: 0.33832213282585144\n",
      "Iter: 80 \tLoss: 0.32551777362823486\n",
      "Iter: 90 \tLoss: 0.3214159607887268\n",
      "Iter: 100 \tLoss: 0.32505959272384644\n",
      "Iter: 110 \tLoss: 0.30771249532699585\n",
      "Mean Train Loss: 0.318008\n",
      "\n",
      "Epoch: 35\n",
      "Iter: 0 \tLoss: 0.29798686504364014\n",
      "Iter: 10 \tLoss: 0.27046096324920654\n",
      "Iter: 20 \tLoss: 0.3490828275680542\n",
      "Iter: 30 \tLoss: 0.2998022139072418\n",
      "Iter: 40 \tLoss: 0.3448180854320526\n",
      "Iter: 50 \tLoss: 0.3341580629348755\n",
      "Iter: 60 \tLoss: 0.31516432762145996\n",
      "Iter: 70 \tLoss: 0.28583234548568726\n",
      "Iter: 80 \tLoss: 0.32122528553009033\n",
      "Iter: 90 \tLoss: 0.3209594488143921\n",
      "Iter: 100 \tLoss: 0.3252221345901489\n",
      "Iter: 110 \tLoss: 0.30391308665275574\n",
      "Mean Train Loss: 0.316085\n",
      "\n",
      "Epoch: 36\n",
      "Iter: 0 \tLoss: 0.3026861250400543\n",
      "Iter: 10 \tLoss: 0.3162102699279785\n",
      "Iter: 20 \tLoss: 0.32141271233558655\n",
      "Iter: 30 \tLoss: 0.2912047207355499\n",
      "Iter: 40 \tLoss: 0.32847192883491516\n",
      "Iter: 50 \tLoss: 0.3513326942920685\n",
      "Iter: 60 \tLoss: 0.2917860150337219\n",
      "Iter: 70 \tLoss: 0.315970778465271\n",
      "Iter: 80 \tLoss: 0.3307039141654968\n",
      "Iter: 90 \tLoss: 0.3100670278072357\n",
      "Iter: 100 \tLoss: 0.3203028440475464\n",
      "Iter: 110 \tLoss: 0.32926374673843384\n",
      "Mean Train Loss: 0.311002\n",
      "\n",
      "Epoch: 37\n",
      "Iter: 0 \tLoss: 0.2980562150478363\n",
      "Iter: 10 \tLoss: 0.315500944852829\n",
      "Iter: 20 \tLoss: 0.33607324957847595\n",
      "Iter: 30 \tLoss: 0.2802813947200775\n",
      "Iter: 40 \tLoss: 0.2962579131126404\n",
      "Iter: 50 \tLoss: 0.30727019906044006\n",
      "Iter: 60 \tLoss: 0.30449748039245605\n",
      "Iter: 70 \tLoss: 0.281035840511322\n",
      "Iter: 80 \tLoss: 0.30446282029151917\n",
      "Iter: 90 \tLoss: 0.3464488685131073\n",
      "Iter: 100 \tLoss: 0.2978183627128601\n",
      "Iter: 110 \tLoss: 0.3382916748523712\n",
      "Mean Train Loss: 0.312066\n",
      "\n",
      "Epoch: 38\n",
      "Iter: 0 \tLoss: 0.3240053057670593\n",
      "Iter: 10 \tLoss: 0.2995312213897705\n",
      "Iter: 20 \tLoss: 0.29699933528900146\n",
      "Iter: 30 \tLoss: 0.2836441099643707\n",
      "Iter: 40 \tLoss: 0.3502323627471924\n",
      "Iter: 50 \tLoss: 0.2800668776035309\n",
      "Iter: 60 \tLoss: 0.3272848129272461\n",
      "Iter: 70 \tLoss: 0.35768601298332214\n",
      "Iter: 80 \tLoss: 0.28577882051467896\n",
      "Iter: 90 \tLoss: 0.33906295895576477\n",
      "Iter: 100 \tLoss: 0.24957776069641113\n",
      "Iter: 110 \tLoss: 0.30720919370651245\n",
      "Mean Train Loss: 0.306517\n",
      "\n",
      "Epoch: 39\n",
      "Iter: 0 \tLoss: 0.30221080780029297\n",
      "Iter: 10 \tLoss: 0.2904386818408966\n",
      "Iter: 20 \tLoss: 0.3116903305053711\n",
      "Iter: 30 \tLoss: 0.2982540428638458\n",
      "Iter: 40 \tLoss: 0.29136866331100464\n",
      "Iter: 50 \tLoss: 0.3016592264175415\n",
      "Iter: 60 \tLoss: 0.3171174228191376\n",
      "Iter: 70 \tLoss: 0.3105398118495941\n",
      "Iter: 80 \tLoss: 0.329770028591156\n",
      "Iter: 90 \tLoss: 0.2967410683631897\n",
      "Iter: 100 \tLoss: 0.295510858297348\n",
      "Iter: 110 \tLoss: 0.32642635703086853\n",
      "Mean Train Loss: 0.304062\n",
      "\n",
      "Val Loss: 0.438799 \tAccuracy: 0.8590425531914894\n"
     ]
    }
   ],
   "source": [
    "train(model_mlp, optimizer, loss_f, train_loader, val_loader, n_epoch, val_fre)\n",
    "validate(model_mlp, val_loader, loss_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f6b094",
   "metadata": {},
   "source": [
    "### Модель 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d72ea129",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hid_features, n_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.c2d = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding='same')\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        self.c2d_2 = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding='same')\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "\n",
    "        self.mp2d = torch.nn.MaxPool2d(kernel_size=3, padding=0)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(81, hid_features)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(hid_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.c2d(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.c2d_2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.mp2d(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.flat(x)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a1bc5749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (c2d): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (relu): ReLU()\n",
      "  (c2d_2): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (relu2): ReLU()\n",
      "  (mp2d): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=81, out_features=1024, bias=True)\n",
      "  (relu3): ReLU()\n",
      "  (fc2): Linear(in_features=1024, out_features=47, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_mlp = MLP(in_features=28*28, hid_features=1024, n_classes=n_classes)\n",
    "print(model_mlp)\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_mlp.parameters(), lr=0.001)\n",
    "\n",
    "n_epoch = 80\n",
    "val_fre = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "91a93305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Iter: 0 \tLoss: 3.8529131412506104\n",
      "Iter: 10 \tLoss: 3.8228836059570312\n",
      "Iter: 20 \tLoss: 3.775935173034668\n",
      "Iter: 30 \tLoss: 3.6897621154785156\n",
      "Iter: 40 \tLoss: 3.5021793842315674\n",
      "Iter: 50 \tLoss: 3.206813335418701\n",
      "Iter: 60 \tLoss: 2.9543497562408447\n",
      "Iter: 70 \tLoss: 2.813699722290039\n",
      "Iter: 80 \tLoss: 2.640726089477539\n",
      "Iter: 90 \tLoss: 2.63256573677063\n",
      "Iter: 100 \tLoss: 2.569199323654175\n",
      "Iter: 110 \tLoss: 2.3938071727752686\n",
      "Mean Train Loss: 3.145835\n",
      "\n",
      "Val Loss: 1.807159 \tAccuracy: 0.5387234042553192\n",
      "Epoch: 1\n",
      "Iter: 0 \tLoss: 2.488112688064575\n",
      "Iter: 10 \tLoss: 2.4033946990966797\n",
      "Iter: 20 \tLoss: 2.362412929534912\n",
      "Iter: 30 \tLoss: 2.329355239868164\n",
      "Iter: 40 \tLoss: 2.3054630756378174\n",
      "Iter: 50 \tLoss: 2.2855300903320312\n",
      "Iter: 60 \tLoss: 2.275346279144287\n",
      "Iter: 70 \tLoss: 2.1301660537719727\n",
      "Iter: 80 \tLoss: 2.1519827842712402\n",
      "Iter: 90 \tLoss: 2.0696325302124023\n",
      "Iter: 100 \tLoss: 2.1096444129943848\n",
      "Iter: 110 \tLoss: 2.11313796043396\n",
      "Mean Train Loss: 2.252274\n",
      "\n",
      "Epoch: 2\n",
      "Iter: 0 \tLoss: 2.108152151107788\n",
      "Iter: 10 \tLoss: 2.0329465866088867\n",
      "Iter: 20 \tLoss: 2.02182674407959\n",
      "Iter: 30 \tLoss: 2.0778186321258545\n",
      "Iter: 40 \tLoss: 2.020604133605957\n",
      "Iter: 50 \tLoss: 1.9573633670806885\n",
      "Iter: 60 \tLoss: 1.953974723815918\n",
      "Iter: 70 \tLoss: 1.9174162149429321\n",
      "Iter: 80 \tLoss: 1.9688529968261719\n",
      "Iter: 90 \tLoss: 1.9457361698150635\n",
      "Iter: 100 \tLoss: 1.8932207822799683\n",
      "Iter: 110 \tLoss: 1.8290096521377563\n",
      "Mean Train Loss: 1.987041\n",
      "\n",
      "Epoch: 3\n",
      "Iter: 0 \tLoss: 1.944353461265564\n",
      "Iter: 10 \tLoss: 1.911943793296814\n",
      "Iter: 20 \tLoss: 1.8253084421157837\n",
      "Iter: 30 \tLoss: 1.7878447771072388\n",
      "Iter: 40 \tLoss: 1.758398413658142\n",
      "Iter: 50 \tLoss: 1.8517898321151733\n",
      "Iter: 60 \tLoss: 1.8259528875350952\n",
      "Iter: 70 \tLoss: 1.768304467201233\n",
      "Iter: 80 \tLoss: 1.8097691535949707\n",
      "Iter: 90 \tLoss: 1.8472108840942383\n",
      "Iter: 100 \tLoss: 1.7489196062088013\n",
      "Iter: 110 \tLoss: 1.7527358531951904\n",
      "Mean Train Loss: 1.826919\n",
      "\n",
      "Epoch: 4\n",
      "Iter: 0 \tLoss: 1.788217306137085\n",
      "Iter: 10 \tLoss: 1.6508599519729614\n",
      "Iter: 20 \tLoss: 1.7016611099243164\n",
      "Iter: 30 \tLoss: 1.732866644859314\n",
      "Iter: 40 \tLoss: 1.7006542682647705\n",
      "Iter: 50 \tLoss: 1.7520760297775269\n",
      "Iter: 60 \tLoss: 1.6159909963607788\n",
      "Iter: 70 \tLoss: 1.6911875009536743\n",
      "Iter: 80 \tLoss: 1.7433843612670898\n",
      "Iter: 90 \tLoss: 1.7380081415176392\n",
      "Iter: 100 \tLoss: 1.6473784446716309\n",
      "Iter: 110 \tLoss: 1.6063811779022217\n",
      "Mean Train Loss: 1.722239\n",
      "\n",
      "Epoch: 5\n",
      "Iter: 0 \tLoss: 1.636969804763794\n",
      "Iter: 10 \tLoss: 1.6723896265029907\n",
      "Iter: 20 \tLoss: 1.6564381122589111\n",
      "Iter: 30 \tLoss: 1.579474687576294\n",
      "Iter: 40 \tLoss: 1.6660219430923462\n",
      "Iter: 50 \tLoss: 1.6695278882980347\n",
      "Iter: 60 \tLoss: 1.5836737155914307\n",
      "Iter: 70 \tLoss: 1.5998631715774536\n",
      "Iter: 80 \tLoss: 1.6615208387374878\n",
      "Iter: 90 \tLoss: 1.5630239248275757\n",
      "Iter: 100 \tLoss: 1.6692285537719727\n",
      "Iter: 110 \tLoss: 1.5330891609191895\n",
      "Mean Train Loss: 1.638272\n",
      "\n",
      "Val Loss: 1.155839 \tAccuracy: 0.7002659574468085\n",
      "Epoch: 6\n",
      "Iter: 0 \tLoss: 1.595015287399292\n",
      "Iter: 10 \tLoss: 1.5434306859970093\n",
      "Iter: 20 \tLoss: 1.597402811050415\n",
      "Iter: 30 \tLoss: 1.610334873199463\n",
      "Iter: 40 \tLoss: 1.5749155282974243\n",
      "Iter: 50 \tLoss: 1.486138939857483\n",
      "Iter: 60 \tLoss: 1.611972689628601\n",
      "Iter: 70 \tLoss: 1.5563799142837524\n",
      "Iter: 80 \tLoss: 1.555582046508789\n",
      "Iter: 90 \tLoss: 1.5821866989135742\n",
      "Iter: 100 \tLoss: 1.5333150625228882\n",
      "Iter: 110 \tLoss: 1.5708986520767212\n",
      "Mean Train Loss: 1.573805\n",
      "\n",
      "Epoch: 7\n",
      "Iter: 0 \tLoss: 1.6139079332351685\n",
      "Iter: 10 \tLoss: 1.4974087476730347\n",
      "Iter: 20 \tLoss: 1.5222737789154053\n",
      "Iter: 30 \tLoss: 1.593896508216858\n",
      "Iter: 40 \tLoss: 1.5181382894515991\n",
      "Iter: 50 \tLoss: 1.4866422414779663\n",
      "Iter: 60 \tLoss: 1.5370023250579834\n",
      "Iter: 70 \tLoss: 1.5548964738845825\n",
      "Iter: 80 \tLoss: 1.5112824440002441\n",
      "Iter: 90 \tLoss: 1.382965087890625\n",
      "Iter: 100 \tLoss: 1.5090950727462769\n",
      "Iter: 110 \tLoss: 1.4467267990112305\n",
      "Mean Train Loss: 1.513018\n",
      "\n",
      "Epoch: 8\n",
      "Iter: 0 \tLoss: 1.476836085319519\n",
      "Iter: 10 \tLoss: 1.3190159797668457\n",
      "Iter: 20 \tLoss: 1.3923643827438354\n",
      "Iter: 30 \tLoss: 1.51270592212677\n",
      "Iter: 40 \tLoss: 1.4179770946502686\n",
      "Iter: 50 \tLoss: 1.5102601051330566\n",
      "Iter: 60 \tLoss: 1.4927531480789185\n",
      "Iter: 70 \tLoss: 1.3640215396881104\n",
      "Iter: 80 \tLoss: 1.4263328313827515\n",
      "Iter: 90 \tLoss: 1.3246620893478394\n",
      "Iter: 100 \tLoss: 1.5093737840652466\n",
      "Iter: 110 \tLoss: 1.4572983980178833\n",
      "Mean Train Loss: 1.459767\n",
      "\n",
      "Epoch: 9\n",
      "Iter: 0 \tLoss: 1.4601588249206543\n",
      "Iter: 10 \tLoss: 1.3734140396118164\n",
      "Iter: 20 \tLoss: 1.5272812843322754\n",
      "Iter: 30 \tLoss: 1.4545612335205078\n",
      "Iter: 40 \tLoss: 1.3757829666137695\n",
      "Iter: 50 \tLoss: 1.424987554550171\n",
      "Iter: 60 \tLoss: 1.4310925006866455\n",
      "Iter: 70 \tLoss: 1.3457658290863037\n",
      "Iter: 80 \tLoss: 1.3877414464950562\n",
      "Iter: 90 \tLoss: 1.3426542282104492\n",
      "Iter: 100 \tLoss: 1.4201321601867676\n",
      "Iter: 110 \tLoss: 1.404191255569458\n",
      "Mean Train Loss: 1.410891\n",
      "\n",
      "Epoch: 10\n",
      "Iter: 0 \tLoss: 1.3891953229904175\n",
      "Iter: 10 \tLoss: 1.3172155618667603\n",
      "Iter: 20 \tLoss: 1.4587664604187012\n",
      "Iter: 30 \tLoss: 1.4235870838165283\n",
      "Iter: 40 \tLoss: 1.4701193571090698\n",
      "Iter: 50 \tLoss: 1.3233672380447388\n",
      "Iter: 60 \tLoss: 1.358644723892212\n",
      "Iter: 70 \tLoss: 1.3222684860229492\n",
      "Iter: 80 \tLoss: 1.3814584016799927\n",
      "Iter: 90 \tLoss: 1.4397239685058594\n",
      "Iter: 100 \tLoss: 1.3939849138259888\n",
      "Iter: 110 \tLoss: 1.3099207878112793\n",
      "Mean Train Loss: 1.363240\n",
      "\n",
      "Val Loss: 1.046019 \tAccuracy: 0.7288829787234042\n",
      "Epoch: 11\n",
      "Iter: 0 \tLoss: 1.3489950895309448\n",
      "Iter: 10 \tLoss: 1.3362047672271729\n",
      "Iter: 20 \tLoss: 1.3610291481018066\n",
      "Iter: 30 \tLoss: 1.251501441001892\n",
      "Iter: 40 \tLoss: 1.3150101900100708\n",
      "Iter: 50 \tLoss: 1.3059228658676147\n",
      "Iter: 60 \tLoss: 1.259711503982544\n",
      "Iter: 70 \tLoss: 1.273244857788086\n",
      "Iter: 80 \tLoss: 1.3827910423278809\n",
      "Iter: 90 \tLoss: 1.3531436920166016\n",
      "Iter: 100 \tLoss: 1.3190971612930298\n",
      "Iter: 110 \tLoss: 1.3005059957504272\n",
      "Mean Train Loss: 1.331420\n",
      "\n",
      "Epoch: 12\n",
      "Iter: 0 \tLoss: 1.2438623905181885\n",
      "Iter: 10 \tLoss: 1.2811299562454224\n",
      "Iter: 20 \tLoss: 1.206073522567749\n",
      "Iter: 30 \tLoss: 1.230721116065979\n",
      "Iter: 40 \tLoss: 1.2766821384429932\n",
      "Iter: 50 \tLoss: 1.264601707458496\n",
      "Iter: 60 \tLoss: 1.334245204925537\n",
      "Iter: 70 \tLoss: 1.3085335493087769\n",
      "Iter: 80 \tLoss: 1.3782625198364258\n",
      "Iter: 90 \tLoss: 1.3443394899368286\n",
      "Iter: 100 \tLoss: 1.1955486536026\n",
      "Iter: 110 \tLoss: 1.3203058242797852\n",
      "Mean Train Loss: 1.294532\n",
      "\n",
      "Epoch: 13\n",
      "Iter: 0 \tLoss: 1.3204400539398193\n",
      "Iter: 10 \tLoss: 1.221264362335205\n",
      "Iter: 20 \tLoss: 1.2703009843826294\n",
      "Iter: 30 \tLoss: 1.276564598083496\n",
      "Iter: 40 \tLoss: 1.4228955507278442\n",
      "Iter: 50 \tLoss: 1.2285090684890747\n",
      "Iter: 60 \tLoss: 1.2239264249801636\n",
      "Iter: 70 \tLoss: 1.2612941265106201\n",
      "Iter: 80 \tLoss: 1.246419906616211\n",
      "Iter: 90 \tLoss: 1.2768703699111938\n",
      "Iter: 100 \tLoss: 1.1697553396224976\n",
      "Iter: 110 \tLoss: 1.297793984413147\n",
      "Mean Train Loss: 1.265109\n",
      "\n",
      "Epoch: 14\n",
      "Iter: 0 \tLoss: 1.2267886400222778\n",
      "Iter: 10 \tLoss: 1.2688136100769043\n",
      "Iter: 20 \tLoss: 1.1654789447784424\n",
      "Iter: 30 \tLoss: 1.2935651540756226\n",
      "Iter: 40 \tLoss: 1.2863352298736572\n",
      "Iter: 50 \tLoss: 1.2372833490371704\n",
      "Iter: 60 \tLoss: 1.25495445728302\n",
      "Iter: 70 \tLoss: 1.2431650161743164\n",
      "Iter: 80 \tLoss: 1.2893376350402832\n",
      "Iter: 90 \tLoss: 1.2716991901397705\n",
      "Iter: 100 \tLoss: 1.187556505203247\n",
      "Iter: 110 \tLoss: 1.187745451927185\n",
      "Mean Train Loss: 1.237330\n",
      "\n",
      "Epoch: 15\n",
      "Iter: 0 \tLoss: 1.1704928874969482\n",
      "Iter: 10 \tLoss: 1.2632498741149902\n",
      "Iter: 20 \tLoss: 1.2638280391693115\n",
      "Iter: 30 \tLoss: 1.2531371116638184\n",
      "Iter: 40 \tLoss: 1.3116583824157715\n",
      "Iter: 50 \tLoss: 1.2372441291809082\n",
      "Iter: 60 \tLoss: 1.2132198810577393\n",
      "Iter: 70 \tLoss: 1.2114107608795166\n",
      "Iter: 80 \tLoss: 1.18419349193573\n",
      "Iter: 90 \tLoss: 1.1997835636138916\n",
      "Iter: 100 \tLoss: 1.2188018560409546\n",
      "Iter: 110 \tLoss: 1.1425659656524658\n",
      "Mean Train Loss: 1.213079\n",
      "\n",
      "Val Loss: 1.040951 \tAccuracy: 0.7130851063829787\n",
      "Epoch: 16\n",
      "Iter: 0 \tLoss: 1.170547366142273\n",
      "Iter: 10 \tLoss: 1.091866374015808\n",
      "Iter: 20 \tLoss: 1.2417445182800293\n",
      "Iter: 30 \tLoss: 1.1206713914871216\n",
      "Iter: 40 \tLoss: 1.1621856689453125\n",
      "Iter: 50 \tLoss: 1.1627155542373657\n",
      "Iter: 60 \tLoss: 1.178392767906189\n",
      "Iter: 70 \tLoss: 1.1507914066314697\n",
      "Iter: 80 \tLoss: 1.1975436210632324\n",
      "Iter: 90 \tLoss: 1.1992367506027222\n",
      "Iter: 100 \tLoss: 1.1951298713684082\n",
      "Iter: 110 \tLoss: 1.1437984704971313\n",
      "Mean Train Loss: 1.191145\n",
      "\n",
      "Epoch: 17\n",
      "Iter: 0 \tLoss: 1.2788914442062378\n",
      "Iter: 10 \tLoss: 1.1240946054458618\n",
      "Iter: 20 \tLoss: 1.1221390962600708\n",
      "Iter: 30 \tLoss: 1.1767990589141846\n",
      "Iter: 40 \tLoss: 1.1611241102218628\n",
      "Iter: 50 \tLoss: 1.2510863542556763\n",
      "Iter: 60 \tLoss: 1.2332881689071655\n",
      "Iter: 70 \tLoss: 1.1502771377563477\n",
      "Iter: 80 \tLoss: 1.1408432722091675\n",
      "Iter: 90 \tLoss: 1.1585205793380737\n",
      "Iter: 100 \tLoss: 1.1262805461883545\n",
      "Iter: 110 \tLoss: 1.1129400730133057\n",
      "Mean Train Loss: 1.177384\n",
      "\n",
      "Epoch: 18\n",
      "Iter: 0 \tLoss: 1.2707006931304932\n",
      "Iter: 10 \tLoss: 1.1912779808044434\n",
      "Iter: 20 \tLoss: 1.1539500951766968\n",
      "Iter: 30 \tLoss: 1.1535662412643433\n",
      "Iter: 40 \tLoss: 1.1256740093231201\n",
      "Iter: 50 \tLoss: 1.1081336736679077\n",
      "Iter: 60 \tLoss: 1.131999135017395\n",
      "Iter: 70 \tLoss: 1.2161732912063599\n",
      "Iter: 80 \tLoss: 1.1426541805267334\n",
      "Iter: 90 \tLoss: 1.1620368957519531\n",
      "Iter: 100 \tLoss: 1.2842161655426025\n",
      "Iter: 110 \tLoss: 1.1832661628723145\n",
      "Mean Train Loss: 1.156377\n",
      "\n",
      "Epoch: 19\n",
      "Iter: 0 \tLoss: 1.1649941205978394\n",
      "Iter: 10 \tLoss: 1.0832754373550415\n",
      "Iter: 20 \tLoss: 1.1999636888504028\n",
      "Iter: 30 \tLoss: 1.1973156929016113\n",
      "Iter: 40 \tLoss: 1.1681329011917114\n",
      "Iter: 50 \tLoss: 1.2187550067901611\n",
      "Iter: 60 \tLoss: 1.129604458808899\n",
      "Iter: 70 \tLoss: 1.1334331035614014\n",
      "Iter: 80 \tLoss: 1.1069847345352173\n",
      "Iter: 90 \tLoss: 1.156636118888855\n",
      "Iter: 100 \tLoss: 1.1425601243972778\n",
      "Iter: 110 \tLoss: 1.1515040397644043\n",
      "Mean Train Loss: 1.151521\n",
      "\n",
      "Epoch: 20\n",
      "Iter: 0 \tLoss: 1.130324125289917\n",
      "Iter: 10 \tLoss: 1.0836910009384155\n",
      "Iter: 20 \tLoss: 1.1224039793014526\n",
      "Iter: 30 \tLoss: 1.0971219539642334\n",
      "Iter: 40 \tLoss: 1.076923131942749\n",
      "Iter: 50 \tLoss: 1.1704527139663696\n",
      "Iter: 60 \tLoss: 1.1067495346069336\n",
      "Iter: 70 \tLoss: 1.043541431427002\n",
      "Iter: 80 \tLoss: 1.1185345649719238\n",
      "Iter: 90 \tLoss: 1.1027802228927612\n",
      "Iter: 100 \tLoss: 1.1491342782974243\n",
      "Iter: 110 \tLoss: 1.1741688251495361\n",
      "Mean Train Loss: 1.126511\n",
      "\n",
      "Val Loss: 1.053227 \tAccuracy: 0.6990957446808511\n",
      "Epoch: 21\n",
      "Iter: 0 \tLoss: 1.1226017475128174\n",
      "Iter: 10 \tLoss: 1.123221516609192\n",
      "Iter: 20 \tLoss: 1.1809628009796143\n",
      "Iter: 30 \tLoss: 1.1450661420822144\n",
      "Iter: 40 \tLoss: 1.1122748851776123\n",
      "Iter: 50 \tLoss: 1.1347901821136475\n",
      "Iter: 60 \tLoss: 1.0545496940612793\n",
      "Iter: 70 \tLoss: 1.1873806715011597\n",
      "Iter: 80 \tLoss: 1.1685824394226074\n",
      "Iter: 90 \tLoss: 1.0492736101150513\n",
      "Iter: 100 \tLoss: 1.1360851526260376\n",
      "Iter: 110 \tLoss: 1.1008316278457642\n",
      "Mean Train Loss: 1.119064\n",
      "\n",
      "Epoch: 22\n",
      "Iter: 0 \tLoss: 1.0872750282287598\n",
      "Iter: 10 \tLoss: 1.171697974205017\n",
      "Iter: 20 \tLoss: 1.118826150894165\n",
      "Iter: 30 \tLoss: 1.1783710718154907\n",
      "Iter: 40 \tLoss: 1.060704231262207\n",
      "Iter: 50 \tLoss: 1.1034541130065918\n",
      "Iter: 60 \tLoss: 1.0630499124526978\n",
      "Iter: 70 \tLoss: 1.180967092514038\n",
      "Iter: 80 \tLoss: 1.1303662061691284\n",
      "Iter: 90 \tLoss: 1.1660834550857544\n",
      "Iter: 100 \tLoss: 1.0813487768173218\n",
      "Iter: 110 \tLoss: 1.110772967338562\n",
      "Mean Train Loss: 1.107337\n",
      "\n",
      "Epoch: 23\n",
      "Iter: 0 \tLoss: 1.0711759328842163\n",
      "Iter: 10 \tLoss: 1.135910987854004\n",
      "Iter: 20 \tLoss: 1.0377978086471558\n",
      "Iter: 30 \tLoss: 1.1643403768539429\n",
      "Iter: 40 \tLoss: 1.139235258102417\n",
      "Iter: 50 \tLoss: 1.1406378746032715\n",
      "Iter: 60 \tLoss: 1.0360981225967407\n",
      "Iter: 70 \tLoss: 1.1668692827224731\n",
      "Iter: 80 \tLoss: 1.045044183731079\n",
      "Iter: 90 \tLoss: 1.0828759670257568\n",
      "Iter: 100 \tLoss: 0.9767798781394958\n",
      "Iter: 110 \tLoss: 1.1398591995239258\n",
      "Mean Train Loss: 1.102826\n",
      "\n",
      "Epoch: 24\n",
      "Iter: 0 \tLoss: 1.0810078382492065\n",
      "Iter: 10 \tLoss: 1.0425856113433838\n",
      "Iter: 20 \tLoss: 1.1207213401794434\n",
      "Iter: 30 \tLoss: 1.1062138080596924\n",
      "Iter: 40 \tLoss: 1.0922250747680664\n",
      "Iter: 50 \tLoss: 1.077973484992981\n",
      "Iter: 60 \tLoss: 1.0887384414672852\n",
      "Iter: 70 \tLoss: 1.127133846282959\n",
      "Iter: 80 \tLoss: 1.0151822566986084\n",
      "Iter: 90 \tLoss: 0.9949333071708679\n",
      "Iter: 100 \tLoss: 1.0663448572158813\n",
      "Iter: 110 \tLoss: 1.0983854532241821\n",
      "Mean Train Loss: 1.091805\n",
      "\n",
      "Epoch: 25\n",
      "Iter: 0 \tLoss: 1.0487135648727417\n",
      "Iter: 10 \tLoss: 1.0921909809112549\n",
      "Iter: 20 \tLoss: 1.0852619409561157\n",
      "Iter: 30 \tLoss: 1.1200287342071533\n",
      "Iter: 40 \tLoss: 1.0699714422225952\n",
      "Iter: 50 \tLoss: 1.1090998649597168\n",
      "Iter: 60 \tLoss: 1.0783987045288086\n",
      "Iter: 70 \tLoss: 1.0516061782836914\n",
      "Iter: 80 \tLoss: 1.0348994731903076\n",
      "Iter: 90 \tLoss: 1.0999611616134644\n",
      "Iter: 100 \tLoss: 1.0802063941955566\n",
      "Iter: 110 \tLoss: 1.0517936944961548\n",
      "Mean Train Loss: 1.080096\n",
      "\n",
      "Val Loss: 1.160587 \tAccuracy: 0.6520212765957447\n",
      "Epoch: 26\n",
      "Iter: 0 \tLoss: 1.0861564874649048\n",
      "Iter: 10 \tLoss: 1.0491318702697754\n",
      "Iter: 20 \tLoss: 1.0899434089660645\n",
      "Iter: 30 \tLoss: 1.0285786390304565\n",
      "Iter: 40 \tLoss: 1.0995686054229736\n",
      "Iter: 50 \tLoss: 1.0148687362670898\n",
      "Iter: 60 \tLoss: 1.0419329404830933\n",
      "Iter: 70 \tLoss: 1.007481336593628\n",
      "Iter: 80 \tLoss: 1.1130716800689697\n",
      "Iter: 90 \tLoss: 1.0192453861236572\n",
      "Iter: 100 \tLoss: 1.0558276176452637\n",
      "Iter: 110 \tLoss: 1.0488423109054565\n",
      "Mean Train Loss: 1.066409\n",
      "\n",
      "Epoch: 27\n",
      "Iter: 0 \tLoss: 1.0411499738693237\n",
      "Iter: 10 \tLoss: 1.1483582258224487\n",
      "Iter: 20 \tLoss: 1.0909316539764404\n",
      "Iter: 30 \tLoss: 1.0199193954467773\n",
      "Iter: 40 \tLoss: 1.0391381978988647\n",
      "Iter: 50 \tLoss: 1.0080485343933105\n",
      "Iter: 60 \tLoss: 1.029308557510376\n",
      "Iter: 70 \tLoss: 0.9781357049942017\n",
      "Iter: 80 \tLoss: 1.0319041013717651\n",
      "Iter: 90 \tLoss: 1.1028056144714355\n",
      "Iter: 100 \tLoss: 1.0708250999450684\n",
      "Iter: 110 \tLoss: 1.018528699874878\n",
      "Mean Train Loss: 1.061966\n",
      "\n",
      "Epoch: 28\n",
      "Iter: 0 \tLoss: 1.0519872903823853\n",
      "Iter: 10 \tLoss: 0.9948223233222961\n",
      "Iter: 20 \tLoss: 1.092075228691101\n",
      "Iter: 30 \tLoss: 1.0307577848434448\n",
      "Iter: 40 \tLoss: 1.0910978317260742\n",
      "Iter: 50 \tLoss: 1.0396512746810913\n",
      "Iter: 60 \tLoss: 1.0972660779953003\n",
      "Iter: 70 \tLoss: 1.080436110496521\n",
      "Iter: 80 \tLoss: 1.0903074741363525\n",
      "Iter: 90 \tLoss: 1.0803340673446655\n",
      "Iter: 100 \tLoss: 0.9890965819358826\n",
      "Iter: 110 \tLoss: 1.0438134670257568\n",
      "Mean Train Loss: 1.058939\n",
      "\n",
      "Epoch: 29\n",
      "Iter: 0 \tLoss: 1.0392909049987793\n",
      "Iter: 10 \tLoss: 1.0044918060302734\n",
      "Iter: 20 \tLoss: 1.041224479675293\n",
      "Iter: 30 \tLoss: 1.0730924606323242\n",
      "Iter: 40 \tLoss: 0.9832035303115845\n",
      "Iter: 50 \tLoss: 1.014025092124939\n",
      "Iter: 60 \tLoss: 1.0145082473754883\n",
      "Iter: 70 \tLoss: 1.0796464681625366\n",
      "Iter: 80 \tLoss: 1.0925343036651611\n",
      "Iter: 90 \tLoss: 1.1019237041473389\n",
      "Iter: 100 \tLoss: 1.0651178359985352\n",
      "Iter: 110 \tLoss: 1.098783254623413\n",
      "Mean Train Loss: 1.050109\n",
      "\n",
      "Epoch: 30\n",
      "Iter: 0 \tLoss: 1.0051660537719727\n",
      "Iter: 10 \tLoss: 1.0635229349136353\n",
      "Iter: 20 \tLoss: 1.0600565671920776\n",
      "Iter: 30 \tLoss: 1.0555075407028198\n",
      "Iter: 40 \tLoss: 1.0700223445892334\n",
      "Iter: 50 \tLoss: 0.9996960163116455\n",
      "Iter: 60 \tLoss: 1.068301796913147\n",
      "Iter: 70 \tLoss: 1.1305968761444092\n",
      "Iter: 80 \tLoss: 1.076706051826477\n",
      "Iter: 90 \tLoss: 1.0528239011764526\n",
      "Iter: 100 \tLoss: 1.048461675643921\n",
      "Iter: 110 \tLoss: 0.9561094641685486\n",
      "Mean Train Loss: 1.047044\n",
      "\n",
      "Val Loss: 1.147500 \tAccuracy: 0.6614893617021277\n",
      "Epoch: 31\n",
      "Iter: 0 \tLoss: 1.0700660943984985\n",
      "Iter: 10 \tLoss: 0.987156867980957\n",
      "Iter: 20 \tLoss: 1.1061058044433594\n",
      "Iter: 30 \tLoss: 0.9896319508552551\n",
      "Iter: 40 \tLoss: 1.080161690711975\n",
      "Iter: 50 \tLoss: 1.030163049697876\n",
      "Iter: 60 \tLoss: 1.0034689903259277\n",
      "Iter: 70 \tLoss: 1.0129168033599854\n",
      "Iter: 80 \tLoss: 1.0348016023635864\n",
      "Iter: 90 \tLoss: 1.063553810119629\n",
      "Iter: 100 \tLoss: 1.0343989133834839\n",
      "Iter: 110 \tLoss: 1.036964774131775\n",
      "Mean Train Loss: 1.040072\n",
      "\n",
      "Epoch: 32\n",
      "Iter: 0 \tLoss: 1.0276997089385986\n",
      "Iter: 10 \tLoss: 1.036605954170227\n",
      "Iter: 20 \tLoss: 1.0215859413146973\n",
      "Iter: 30 \tLoss: 0.9941202402114868\n",
      "Iter: 40 \tLoss: 1.009604811668396\n",
      "Iter: 50 \tLoss: 0.9939420819282532\n",
      "Iter: 60 \tLoss: 0.9996678829193115\n",
      "Iter: 70 \tLoss: 0.9997335076332092\n",
      "Iter: 80 \tLoss: 0.9705804586410522\n",
      "Iter: 90 \tLoss: 1.083601713180542\n",
      "Iter: 100 \tLoss: 1.0266392230987549\n",
      "Iter: 110 \tLoss: 1.0200872421264648\n",
      "Mean Train Loss: 1.029610\n",
      "\n",
      "Epoch: 33\n",
      "Iter: 0 \tLoss: 0.9568126201629639\n",
      "Iter: 10 \tLoss: 1.113541841506958\n",
      "Iter: 20 \tLoss: 1.0692005157470703\n",
      "Iter: 30 \tLoss: 1.061710238456726\n",
      "Iter: 40 \tLoss: 0.920844316482544\n",
      "Iter: 50 \tLoss: 0.9610795378684998\n",
      "Iter: 60 \tLoss: 1.0171016454696655\n",
      "Iter: 70 \tLoss: 0.9695498943328857\n",
      "Iter: 80 \tLoss: 1.0421162843704224\n",
      "Iter: 90 \tLoss: 1.022962212562561\n",
      "Iter: 100 \tLoss: 1.0854326486587524\n",
      "Iter: 110 \tLoss: 1.0457255840301514\n",
      "Mean Train Loss: 1.028907\n",
      "\n",
      "Epoch: 34\n",
      "Iter: 0 \tLoss: 1.0104949474334717\n",
      "Iter: 10 \tLoss: 0.9898048043251038\n",
      "Iter: 20 \tLoss: 1.0173789262771606\n",
      "Iter: 30 \tLoss: 0.9541823863983154\n",
      "Iter: 40 \tLoss: 1.0593730211257935\n",
      "Iter: 50 \tLoss: 1.1307320594787598\n",
      "Iter: 60 \tLoss: 1.0512052774429321\n",
      "Iter: 70 \tLoss: 1.019566535949707\n",
      "Iter: 80 \tLoss: 1.0471128225326538\n",
      "Iter: 90 \tLoss: 1.0091432332992554\n",
      "Iter: 100 \tLoss: 0.9772421717643738\n",
      "Iter: 110 \tLoss: 1.0749108791351318\n",
      "Mean Train Loss: 1.019454\n",
      "\n",
      "Epoch: 35\n",
      "Iter: 0 \tLoss: 0.9782249927520752\n",
      "Iter: 10 \tLoss: 1.0673846006393433\n",
      "Iter: 20 \tLoss: 0.9110819101333618\n",
      "Iter: 30 \tLoss: 1.0374101400375366\n",
      "Iter: 40 \tLoss: 0.9971834421157837\n",
      "Iter: 50 \tLoss: 1.0647937059402466\n",
      "Iter: 60 \tLoss: 1.0097415447235107\n",
      "Iter: 70 \tLoss: 1.005509376525879\n",
      "Iter: 80 \tLoss: 1.1105492115020752\n",
      "Iter: 90 \tLoss: 1.0639458894729614\n",
      "Iter: 100 \tLoss: 0.9898305535316467\n",
      "Iter: 110 \tLoss: 1.0452799797058105\n",
      "Mean Train Loss: 1.014780\n",
      "\n",
      "Val Loss: 1.259109 \tAccuracy: 0.6142553191489362\n",
      "Epoch: 36\n",
      "Iter: 0 \tLoss: 1.0103769302368164\n",
      "Iter: 10 \tLoss: 1.00710928440094\n",
      "Iter: 20 \tLoss: 1.0350353717803955\n",
      "Iter: 30 \tLoss: 1.0792672634124756\n",
      "Iter: 40 \tLoss: 1.0390698909759521\n",
      "Iter: 50 \tLoss: 1.0740035772323608\n",
      "Iter: 60 \tLoss: 1.0298893451690674\n",
      "Iter: 70 \tLoss: 1.1230690479278564\n",
      "Iter: 80 \tLoss: 1.0611010789871216\n",
      "Iter: 90 \tLoss: 1.088609218597412\n",
      "Iter: 100 \tLoss: 1.0262882709503174\n",
      "Iter: 110 \tLoss: 0.9236350059509277\n",
      "Mean Train Loss: 1.016714\n",
      "\n",
      "Epoch: 37\n",
      "Iter: 0 \tLoss: 1.0171998739242554\n",
      "Iter: 10 \tLoss: 1.0585365295410156\n",
      "Iter: 20 \tLoss: 0.9898924231529236\n",
      "Iter: 30 \tLoss: 1.0218982696533203\n",
      "Iter: 40 \tLoss: 1.034607172012329\n",
      "Iter: 50 \tLoss: 1.00653076171875\n",
      "Iter: 60 \tLoss: 1.0513055324554443\n",
      "Iter: 70 \tLoss: 1.0937812328338623\n",
      "Iter: 80 \tLoss: 1.0280762910842896\n",
      "Iter: 90 \tLoss: 0.9633507132530212\n",
      "Iter: 100 \tLoss: 0.9828202724456787\n",
      "Iter: 110 \tLoss: 0.9472844004631042\n",
      "Mean Train Loss: 1.007862\n",
      "\n",
      "Epoch: 38\n",
      "Iter: 0 \tLoss: 0.9488234519958496\n",
      "Iter: 10 \tLoss: 1.0227930545806885\n",
      "Iter: 20 \tLoss: 1.0475643873214722\n",
      "Iter: 30 \tLoss: 1.004553198814392\n",
      "Iter: 40 \tLoss: 0.9263971447944641\n",
      "Iter: 50 \tLoss: 1.043715476989746\n",
      "Iter: 60 \tLoss: 1.0412136316299438\n",
      "Iter: 70 \tLoss: 0.9859333634376526\n",
      "Iter: 80 \tLoss: 1.038977861404419\n",
      "Iter: 90 \tLoss: 0.9663052558898926\n",
      "Iter: 100 \tLoss: 0.9864031076431274\n",
      "Iter: 110 \tLoss: 1.0181496143341064\n",
      "Mean Train Loss: 1.006342\n",
      "\n",
      "Epoch: 39\n",
      "Iter: 0 \tLoss: 0.8556316494941711\n",
      "Iter: 10 \tLoss: 0.9840198755264282\n",
      "Iter: 20 \tLoss: 1.0127652883529663\n",
      "Iter: 30 \tLoss: 1.026699185371399\n",
      "Iter: 40 \tLoss: 0.9861583709716797\n",
      "Iter: 50 \tLoss: 0.9524581432342529\n",
      "Iter: 60 \tLoss: 0.9671084880828857\n",
      "Iter: 70 \tLoss: 0.9923563003540039\n",
      "Iter: 80 \tLoss: 1.0926166772842407\n",
      "Iter: 90 \tLoss: 1.0250295400619507\n",
      "Iter: 100 \tLoss: 0.9915102124214172\n",
      "Iter: 110 \tLoss: 1.0006508827209473\n",
      "Mean Train Loss: 0.994518\n",
      "\n",
      "Epoch: 40\n",
      "Iter: 0 \tLoss: 0.94451504945755\n",
      "Iter: 10 \tLoss: 1.0328845977783203\n",
      "Iter: 20 \tLoss: 1.0132962465286255\n",
      "Iter: 30 \tLoss: 1.0102901458740234\n",
      "Iter: 40 \tLoss: 1.026658296585083\n",
      "Iter: 50 \tLoss: 1.0019428730010986\n",
      "Iter: 60 \tLoss: 0.9733483195304871\n",
      "Iter: 70 \tLoss: 1.0630422830581665\n",
      "Iter: 80 \tLoss: 1.0927748680114746\n",
      "Iter: 90 \tLoss: 0.9453592896461487\n",
      "Iter: 100 \tLoss: 1.0563586950302124\n",
      "Iter: 110 \tLoss: 1.0617949962615967\n",
      "Mean Train Loss: 0.991741\n",
      "\n",
      "Val Loss: 1.239503 \tAccuracy: 0.6207978723404255\n",
      "Epoch: 41\n",
      "Iter: 0 \tLoss: 0.9349783658981323\n",
      "Iter: 10 \tLoss: 1.0151968002319336\n",
      "Iter: 20 \tLoss: 1.086386799812317\n",
      "Iter: 30 \tLoss: 0.9995364546775818\n",
      "Iter: 40 \tLoss: 0.9862180352210999\n",
      "Iter: 50 \tLoss: 0.9720205664634705\n",
      "Iter: 60 \tLoss: 0.9705168008804321\n",
      "Iter: 70 \tLoss: 1.0684419870376587\n",
      "Iter: 80 \tLoss: 0.9974316358566284\n",
      "Iter: 90 \tLoss: 0.9792773723602295\n",
      "Iter: 100 \tLoss: 0.9717473983764648\n",
      "Iter: 110 \tLoss: 0.9962750673294067\n",
      "Mean Train Loss: 0.988230\n",
      "\n",
      "Epoch: 42\n",
      "Iter: 0 \tLoss: 0.9802973866462708\n",
      "Iter: 10 \tLoss: 0.9878122806549072\n",
      "Iter: 20 \tLoss: 1.0776071548461914\n",
      "Iter: 30 \tLoss: 1.0541404485702515\n",
      "Iter: 40 \tLoss: 0.9421555399894714\n",
      "Iter: 50 \tLoss: 0.9294360876083374\n",
      "Iter: 60 \tLoss: 0.9999414086341858\n",
      "Iter: 70 \tLoss: 1.0107154846191406\n",
      "Iter: 80 \tLoss: 1.00931978225708\n",
      "Iter: 90 \tLoss: 1.0202648639678955\n",
      "Iter: 100 \tLoss: 1.04082453250885\n",
      "Iter: 110 \tLoss: 1.0169941186904907\n",
      "Mean Train Loss: 0.988723\n",
      "\n",
      "Epoch: 43\n",
      "Iter: 0 \tLoss: 0.9784097075462341\n",
      "Iter: 10 \tLoss: 0.9477746486663818\n",
      "Iter: 20 \tLoss: 0.9905325174331665\n",
      "Iter: 30 \tLoss: 1.0081777572631836\n",
      "Iter: 40 \tLoss: 0.9709333777427673\n",
      "Iter: 50 \tLoss: 0.9707927107810974\n",
      "Iter: 60 \tLoss: 0.9757689237594604\n",
      "Iter: 70 \tLoss: 0.986297607421875\n",
      "Iter: 80 \tLoss: 0.9555255770683289\n",
      "Iter: 90 \tLoss: 0.9861959218978882\n",
      "Iter: 100 \tLoss: 0.9889176487922668\n",
      "Iter: 110 \tLoss: 0.9987813234329224\n",
      "Mean Train Loss: 0.986317\n",
      "\n",
      "Epoch: 44\n",
      "Iter: 0 \tLoss: 0.9426827430725098\n",
      "Iter: 10 \tLoss: 0.9688494801521301\n",
      "Iter: 20 \tLoss: 1.0255693197250366\n",
      "Iter: 30 \tLoss: 0.9506893754005432\n",
      "Iter: 40 \tLoss: 1.0057846307754517\n",
      "Iter: 50 \tLoss: 1.0141239166259766\n",
      "Iter: 60 \tLoss: 1.0706688165664673\n",
      "Iter: 70 \tLoss: 1.048112154006958\n",
      "Iter: 80 \tLoss: 1.0026158094406128\n",
      "Iter: 90 \tLoss: 1.0061793327331543\n",
      "Iter: 100 \tLoss: 0.9698678851127625\n",
      "Iter: 110 \tLoss: 0.988508403301239\n",
      "Mean Train Loss: 0.978432\n",
      "\n",
      "Epoch: 45\n",
      "Iter: 0 \tLoss: 0.9773510098457336\n",
      "Iter: 10 \tLoss: 0.9389567375183105\n",
      "Iter: 20 \tLoss: 0.9533304572105408\n",
      "Iter: 30 \tLoss: 0.9787043333053589\n",
      "Iter: 40 \tLoss: 0.9119389057159424\n",
      "Iter: 50 \tLoss: 0.9722573161125183\n",
      "Iter: 60 \tLoss: 0.9896129369735718\n",
      "Iter: 70 \tLoss: 0.9947382807731628\n",
      "Iter: 80 \tLoss: 0.9313151240348816\n",
      "Iter: 90 \tLoss: 0.9890697598457336\n",
      "Iter: 100 \tLoss: 1.0313096046447754\n",
      "Iter: 110 \tLoss: 0.9615831971168518\n",
      "Mean Train Loss: 0.977649\n",
      "\n",
      "Val Loss: 1.315881 \tAccuracy: 0.5919148936170213\n",
      "Epoch: 46\n",
      "Iter: 0 \tLoss: 0.9646111130714417\n",
      "Iter: 10 \tLoss: 1.0097895860671997\n",
      "Iter: 20 \tLoss: 0.9516100287437439\n",
      "Iter: 30 \tLoss: 0.9102287888526917\n",
      "Iter: 40 \tLoss: 1.0267399549484253\n",
      "Iter: 50 \tLoss: 0.9695249795913696\n",
      "Iter: 60 \tLoss: 0.9454172849655151\n",
      "Iter: 70 \tLoss: 0.9836459159851074\n",
      "Iter: 80 \tLoss: 0.9426608085632324\n",
      "Iter: 90 \tLoss: 0.9831213355064392\n",
      "Iter: 100 \tLoss: 0.9493464827537537\n",
      "Iter: 110 \tLoss: 1.0107260942459106\n",
      "Mean Train Loss: 0.974381\n",
      "\n",
      "Epoch: 47\n",
      "Iter: 0 \tLoss: 0.9783474802970886\n",
      "Iter: 10 \tLoss: 0.9833397269248962\n",
      "Iter: 20 \tLoss: 0.969896674156189\n",
      "Iter: 30 \tLoss: 0.9994223117828369\n",
      "Iter: 40 \tLoss: 0.9556118845939636\n",
      "Iter: 50 \tLoss: 1.0510092973709106\n",
      "Iter: 60 \tLoss: 0.9694116115570068\n",
      "Iter: 70 \tLoss: 0.8993027210235596\n",
      "Iter: 80 \tLoss: 0.9771748781204224\n",
      "Iter: 90 \tLoss: 1.0047111511230469\n",
      "Iter: 100 \tLoss: 1.0020692348480225\n",
      "Iter: 110 \tLoss: 0.9473749399185181\n",
      "Mean Train Loss: 0.972355\n",
      "\n",
      "Epoch: 48\n",
      "Iter: 0 \tLoss: 0.9215167164802551\n",
      "Iter: 10 \tLoss: 0.9324953556060791\n",
      "Iter: 20 \tLoss: 0.9751591682434082\n",
      "Iter: 30 \tLoss: 0.9956700801849365\n",
      "Iter: 40 \tLoss: 0.985526442527771\n",
      "Iter: 50 \tLoss: 1.0059973001480103\n",
      "Iter: 60 \tLoss: 0.9184704422950745\n",
      "Iter: 70 \tLoss: 1.0039793252944946\n",
      "Iter: 80 \tLoss: 0.9868199229240417\n",
      "Iter: 90 \tLoss: 0.8769697546958923\n",
      "Iter: 100 \tLoss: 0.9062907099723816\n",
      "Iter: 110 \tLoss: 0.967078685760498\n",
      "Mean Train Loss: 0.966314\n",
      "\n",
      "Epoch: 49\n",
      "Iter: 0 \tLoss: 0.9348477125167847\n",
      "Iter: 10 \tLoss: 1.02181077003479\n",
      "Iter: 20 \tLoss: 0.9829985499382019\n",
      "Iter: 30 \tLoss: 0.9658433794975281\n",
      "Iter: 40 \tLoss: 0.9233325123786926\n",
      "Iter: 50 \tLoss: 0.9403272271156311\n",
      "Iter: 60 \tLoss: 0.9043164849281311\n",
      "Iter: 70 \tLoss: 0.9874945282936096\n",
      "Iter: 80 \tLoss: 0.9775259494781494\n",
      "Iter: 90 \tLoss: 0.9972887635231018\n",
      "Iter: 100 \tLoss: 1.0422567129135132\n",
      "Iter: 110 \tLoss: 1.0201267004013062\n",
      "Mean Train Loss: 0.966169\n",
      "\n",
      "Epoch: 50\n",
      "Iter: 0 \tLoss: 0.932542622089386\n",
      "Iter: 10 \tLoss: 0.9557211399078369\n",
      "Iter: 20 \tLoss: 0.9781731367111206\n",
      "Iter: 30 \tLoss: 1.0120539665222168\n",
      "Iter: 40 \tLoss: 0.9812076687812805\n",
      "Iter: 50 \tLoss: 1.0278395414352417\n",
      "Iter: 60 \tLoss: 0.9549912214279175\n",
      "Iter: 70 \tLoss: 0.9501283168792725\n",
      "Iter: 80 \tLoss: 0.9709022045135498\n",
      "Iter: 90 \tLoss: 0.9474530816078186\n",
      "Iter: 100 \tLoss: 0.9836603403091431\n",
      "Iter: 110 \tLoss: 0.9842597842216492\n",
      "Mean Train Loss: 0.958495\n",
      "\n",
      "Val Loss: 1.310345 \tAccuracy: 0.5930851063829787\n",
      "Epoch: 51\n",
      "Iter: 0 \tLoss: 0.879128098487854\n",
      "Iter: 10 \tLoss: 0.9382277727127075\n",
      "Iter: 20 \tLoss: 0.912632405757904\n",
      "Iter: 30 \tLoss: 1.0223290920257568\n",
      "Iter: 40 \tLoss: 1.0220277309417725\n",
      "Iter: 50 \tLoss: 0.9640952944755554\n",
      "Iter: 60 \tLoss: 0.9211004972457886\n",
      "Iter: 70 \tLoss: 0.9314035177230835\n",
      "Iter: 80 \tLoss: 0.9361936450004578\n",
      "Iter: 90 \tLoss: 0.9178383350372314\n",
      "Iter: 100 \tLoss: 0.9681800007820129\n",
      "Iter: 110 \tLoss: 0.9863635897636414\n",
      "Mean Train Loss: 0.958172\n",
      "\n",
      "Epoch: 52\n",
      "Iter: 0 \tLoss: 0.9332517981529236\n",
      "Iter: 10 \tLoss: 0.9964004755020142\n",
      "Iter: 20 \tLoss: 0.9617379307746887\n",
      "Iter: 30 \tLoss: 0.9725547432899475\n",
      "Iter: 40 \tLoss: 0.9369834661483765\n",
      "Iter: 50 \tLoss: 0.9444447755813599\n",
      "Iter: 60 \tLoss: 1.0147913694381714\n",
      "Iter: 70 \tLoss: 0.9444312453269958\n",
      "Iter: 80 \tLoss: 0.9890678524971008\n",
      "Iter: 90 \tLoss: 0.9482730627059937\n",
      "Iter: 100 \tLoss: 0.9804363250732422\n",
      "Iter: 110 \tLoss: 0.9517232775688171\n",
      "Mean Train Loss: 0.955950\n",
      "\n",
      "Epoch: 53\n",
      "Iter: 0 \tLoss: 0.9218508005142212\n",
      "Iter: 10 \tLoss: 0.953335165977478\n",
      "Iter: 20 \tLoss: 1.020490050315857\n",
      "Iter: 30 \tLoss: 0.9798282384872437\n",
      "Iter: 40 \tLoss: 0.9782360196113586\n",
      "Iter: 50 \tLoss: 0.9492495656013489\n",
      "Iter: 60 \tLoss: 0.9218423962593079\n",
      "Iter: 70 \tLoss: 0.9191422462463379\n",
      "Iter: 80 \tLoss: 1.017295479774475\n",
      "Iter: 90 \tLoss: 1.0134671926498413\n",
      "Iter: 100 \tLoss: 0.981686532497406\n",
      "Iter: 110 \tLoss: 0.9279510378837585\n",
      "Mean Train Loss: 0.955470\n",
      "\n",
      "Epoch: 54\n",
      "Iter: 0 \tLoss: 0.9599695205688477\n",
      "Iter: 10 \tLoss: 0.8748997449874878\n",
      "Iter: 20 \tLoss: 0.9869781732559204\n",
      "Iter: 30 \tLoss: 0.9023793935775757\n",
      "Iter: 40 \tLoss: 0.9013804197311401\n",
      "Iter: 50 \tLoss: 0.9523725509643555\n",
      "Iter: 60 \tLoss: 0.8808863162994385\n",
      "Iter: 70 \tLoss: 0.9949048757553101\n",
      "Iter: 80 \tLoss: 1.0168178081512451\n",
      "Iter: 90 \tLoss: 0.9342360496520996\n",
      "Iter: 100 \tLoss: 0.9673543572425842\n",
      "Iter: 110 \tLoss: 0.9524496793746948\n",
      "Mean Train Loss: 0.949021\n",
      "\n",
      "Epoch: 55\n",
      "Iter: 0 \tLoss: 0.937571108341217\n",
      "Iter: 10 \tLoss: 0.9309082627296448\n",
      "Iter: 20 \tLoss: 0.928390383720398\n",
      "Iter: 30 \tLoss: 0.9447862505912781\n",
      "Iter: 40 \tLoss: 0.9441877603530884\n",
      "Iter: 50 \tLoss: 0.9598307013511658\n",
      "Iter: 60 \tLoss: 0.9698767066001892\n",
      "Iter: 70 \tLoss: 0.958653450012207\n",
      "Iter: 80 \tLoss: 0.9633799195289612\n",
      "Iter: 90 \tLoss: 0.9702346324920654\n",
      "Iter: 100 \tLoss: 0.8811131119728088\n",
      "Iter: 110 \tLoss: 0.9190608263015747\n",
      "Mean Train Loss: 0.948779\n",
      "\n",
      "Val Loss: 1.302814 \tAccuracy: 0.6004787234042553\n",
      "Epoch: 56\n",
      "Iter: 0 \tLoss: 0.9211922883987427\n",
      "Iter: 10 \tLoss: 0.9166606068611145\n",
      "Iter: 20 \tLoss: 0.9387516379356384\n",
      "Iter: 30 \tLoss: 0.9944387078285217\n",
      "Iter: 40 \tLoss: 0.90737384557724\n",
      "Iter: 50 \tLoss: 1.036767601966858\n",
      "Iter: 60 \tLoss: 0.9413544535636902\n",
      "Iter: 70 \tLoss: 0.9630191326141357\n",
      "Iter: 80 \tLoss: 0.947874128818512\n",
      "Iter: 90 \tLoss: 1.0300819873809814\n",
      "Iter: 100 \tLoss: 0.9372813701629639\n",
      "Iter: 110 \tLoss: 1.002046823501587\n",
      "Mean Train Loss: 0.945051\n",
      "\n",
      "Epoch: 57\n",
      "Iter: 0 \tLoss: 0.9364337921142578\n",
      "Iter: 10 \tLoss: 0.9106548428535461\n",
      "Iter: 20 \tLoss: 0.896247923374176\n",
      "Iter: 30 \tLoss: 0.8610742092132568\n",
      "Iter: 40 \tLoss: 0.9307066798210144\n",
      "Iter: 50 \tLoss: 0.969820499420166\n",
      "Iter: 60 \tLoss: 0.9798839092254639\n",
      "Iter: 70 \tLoss: 1.020254135131836\n",
      "Iter: 80 \tLoss: 0.942080020904541\n",
      "Iter: 90 \tLoss: 0.9019039273262024\n",
      "Iter: 100 \tLoss: 0.9291749000549316\n",
      "Iter: 110 \tLoss: 0.9043780565261841\n",
      "Mean Train Loss: 0.940471\n",
      "\n",
      "Epoch: 58\n",
      "Iter: 0 \tLoss: 0.8982563614845276\n",
      "Iter: 10 \tLoss: 0.9302442669868469\n",
      "Iter: 20 \tLoss: 0.9381330013275146\n",
      "Iter: 30 \tLoss: 0.9696193337440491\n",
      "Iter: 40 \tLoss: 0.9272658228874207\n",
      "Iter: 50 \tLoss: 0.9485059380531311\n",
      "Iter: 60 \tLoss: 0.9580165147781372\n",
      "Iter: 70 \tLoss: 0.9251590967178345\n",
      "Iter: 80 \tLoss: 0.8891450762748718\n",
      "Iter: 90 \tLoss: 0.999695360660553\n",
      "Iter: 100 \tLoss: 0.8932248950004578\n",
      "Iter: 110 \tLoss: 0.976363480091095\n",
      "Mean Train Loss: 0.944055\n",
      "\n",
      "Epoch: 59\n",
      "Iter: 0 \tLoss: 0.9619501233100891\n",
      "Iter: 10 \tLoss: 0.9271148443222046\n",
      "Iter: 20 \tLoss: 0.9324998259544373\n",
      "Iter: 30 \tLoss: 0.9245824813842773\n",
      "Iter: 40 \tLoss: 0.9729330539703369\n",
      "Iter: 50 \tLoss: 0.8881609439849854\n",
      "Iter: 60 \tLoss: 0.9942602515220642\n",
      "Iter: 70 \tLoss: 0.9507325887680054\n",
      "Iter: 80 \tLoss: 0.9465857148170471\n",
      "Iter: 90 \tLoss: 0.9514902830123901\n",
      "Iter: 100 \tLoss: 0.9056999087333679\n",
      "Iter: 110 \tLoss: 0.8980081081390381\n",
      "Mean Train Loss: 0.939750\n",
      "\n",
      "Epoch: 60\n",
      "Iter: 0 \tLoss: 0.9056875705718994\n",
      "Iter: 10 \tLoss: 0.9105291366577148\n",
      "Iter: 20 \tLoss: 0.9776809215545654\n",
      "Iter: 30 \tLoss: 0.9439935088157654\n",
      "Iter: 40 \tLoss: 0.9290558099746704\n",
      "Iter: 50 \tLoss: 0.8737562894821167\n",
      "Iter: 60 \tLoss: 0.8761049509048462\n",
      "Iter: 70 \tLoss: 0.9149287343025208\n",
      "Iter: 80 \tLoss: 0.9303867220878601\n",
      "Iter: 90 \tLoss: 0.9909876585006714\n",
      "Iter: 100 \tLoss: 0.9565741419792175\n",
      "Iter: 110 \tLoss: 0.9740269184112549\n",
      "Mean Train Loss: 0.935916\n",
      "\n",
      "Val Loss: 1.312804 \tAccuracy: 0.5891489361702128\n",
      "Epoch: 61\n",
      "Iter: 0 \tLoss: 0.9361907243728638\n",
      "Iter: 10 \tLoss: 0.9287023544311523\n",
      "Iter: 20 \tLoss: 0.9239055514335632\n",
      "Iter: 30 \tLoss: 0.9550395011901855\n",
      "Iter: 40 \tLoss: 0.9035107493400574\n",
      "Iter: 50 \tLoss: 0.9747761487960815\n",
      "Iter: 60 \tLoss: 0.9977290034294128\n",
      "Iter: 70 \tLoss: 0.9378907084465027\n",
      "Iter: 80 \tLoss: 0.9398343563079834\n",
      "Iter: 90 \tLoss: 0.9760439395904541\n",
      "Iter: 100 \tLoss: 0.9727675914764404\n",
      "Iter: 110 \tLoss: 0.9308887124061584\n",
      "Mean Train Loss: 0.933570\n",
      "\n",
      "Epoch: 62\n",
      "Iter: 0 \tLoss: 0.8788869976997375\n",
      "Iter: 10 \tLoss: 0.9306074380874634\n",
      "Iter: 20 \tLoss: 0.9467998147010803\n",
      "Iter: 30 \tLoss: 0.8965743184089661\n",
      "Iter: 40 \tLoss: 0.9377931952476501\n",
      "Iter: 50 \tLoss: 0.8709560632705688\n",
      "Iter: 60 \tLoss: 0.837310254573822\n",
      "Iter: 70 \tLoss: 0.9066821932792664\n",
      "Iter: 80 \tLoss: 0.8990066051483154\n",
      "Iter: 90 \tLoss: 0.8854638934135437\n",
      "Iter: 100 \tLoss: 0.9295998811721802\n",
      "Iter: 110 \tLoss: 0.9770268201828003\n",
      "Mean Train Loss: 0.932662\n",
      "\n",
      "Epoch: 63\n",
      "Iter: 0 \tLoss: 0.8788700103759766\n",
      "Iter: 10 \tLoss: 0.9063641428947449\n",
      "Iter: 20 \tLoss: 0.9947846531867981\n",
      "Iter: 30 \tLoss: 0.880634069442749\n",
      "Iter: 40 \tLoss: 1.0113235712051392\n",
      "Iter: 50 \tLoss: 0.8940410017967224\n",
      "Iter: 60 \tLoss: 0.9597511887550354\n",
      "Iter: 70 \tLoss: 0.926181972026825\n",
      "Iter: 80 \tLoss: 0.8132005929946899\n",
      "Iter: 90 \tLoss: 0.9354243278503418\n",
      "Iter: 100 \tLoss: 0.9004755020141602\n",
      "Iter: 110 \tLoss: 0.8618383407592773\n",
      "Mean Train Loss: 0.927362\n",
      "\n",
      "Epoch: 64\n",
      "Iter: 0 \tLoss: 0.9627005457878113\n",
      "Iter: 10 \tLoss: 0.9846007227897644\n",
      "Iter: 20 \tLoss: 0.91071617603302\n",
      "Iter: 30 \tLoss: 0.9060790538787842\n",
      "Iter: 40 \tLoss: 0.9953584671020508\n",
      "Iter: 50 \tLoss: 0.9852812886238098\n",
      "Iter: 60 \tLoss: 0.8984894752502441\n",
      "Iter: 70 \tLoss: 0.9493882060050964\n",
      "Iter: 80 \tLoss: 0.955590009689331\n",
      "Iter: 90 \tLoss: 0.9107517600059509\n",
      "Iter: 100 \tLoss: 0.9192402958869934\n",
      "Iter: 110 \tLoss: 0.9189442992210388\n",
      "Mean Train Loss: 0.930854\n",
      "\n",
      "Epoch: 65\n",
      "Iter: 0 \tLoss: 1.008865475654602\n",
      "Iter: 10 \tLoss: 0.8882541060447693\n",
      "Iter: 20 \tLoss: 0.8649726510047913\n",
      "Iter: 30 \tLoss: 0.8544943928718567\n",
      "Iter: 40 \tLoss: 0.9242734909057617\n",
      "Iter: 50 \tLoss: 0.9184006452560425\n",
      "Iter: 60 \tLoss: 0.8911747932434082\n",
      "Iter: 70 \tLoss: 0.9736217856407166\n",
      "Iter: 80 \tLoss: 0.8775628209114075\n",
      "Iter: 90 \tLoss: 0.8706768751144409\n",
      "Iter: 100 \tLoss: 0.9225599765777588\n",
      "Iter: 110 \tLoss: 0.9852576851844788\n",
      "Mean Train Loss: 0.923688\n",
      "\n",
      "Val Loss: 1.381566 \tAccuracy: 0.5707446808510638\n",
      "Epoch: 66\n",
      "Iter: 0 \tLoss: 0.9220324754714966\n",
      "Iter: 10 \tLoss: 0.9135103225708008\n",
      "Iter: 20 \tLoss: 0.8915096521377563\n",
      "Iter: 30 \tLoss: 0.9651153087615967\n",
      "Iter: 40 \tLoss: 0.9295644164085388\n",
      "Iter: 50 \tLoss: 0.9248712658882141\n",
      "Iter: 60 \tLoss: 0.8963430523872375\n",
      "Iter: 70 \tLoss: 0.9632943868637085\n",
      "Iter: 80 \tLoss: 0.8636196851730347\n",
      "Iter: 90 \tLoss: 0.9314106702804565\n",
      "Iter: 100 \tLoss: 0.8942767381668091\n",
      "Iter: 110 \tLoss: 0.9815663695335388\n",
      "Mean Train Loss: 0.925989\n",
      "\n",
      "Epoch: 67\n",
      "Iter: 0 \tLoss: 0.9092244505882263\n",
      "Iter: 10 \tLoss: 0.8964776396751404\n",
      "Iter: 20 \tLoss: 0.9457922577857971\n",
      "Iter: 30 \tLoss: 0.9279273152351379\n",
      "Iter: 40 \tLoss: 0.861217737197876\n",
      "Iter: 50 \tLoss: 0.9278766512870789\n",
      "Iter: 60 \tLoss: 0.8672866225242615\n",
      "Iter: 70 \tLoss: 0.8890838623046875\n",
      "Iter: 80 \tLoss: 0.9390164613723755\n",
      "Iter: 90 \tLoss: 0.9780665040016174\n",
      "Iter: 100 \tLoss: 0.9294396042823792\n",
      "Iter: 110 \tLoss: 0.9636927843093872\n",
      "Mean Train Loss: 0.920356\n",
      "\n",
      "Epoch: 68\n",
      "Iter: 0 \tLoss: 0.8782534599304199\n",
      "Iter: 10 \tLoss: 0.8752309679985046\n",
      "Iter: 20 \tLoss: 0.8944708108901978\n",
      "Iter: 30 \tLoss: 0.9426000714302063\n",
      "Iter: 40 \tLoss: 0.9419576525688171\n",
      "Iter: 50 \tLoss: 0.9386500120162964\n",
      "Iter: 60 \tLoss: 0.9833189845085144\n",
      "Iter: 70 \tLoss: 0.9057994484901428\n",
      "Iter: 80 \tLoss: 0.945723295211792\n",
      "Iter: 90 \tLoss: 0.9346177577972412\n",
      "Iter: 100 \tLoss: 0.9722792506217957\n",
      "Iter: 110 \tLoss: 0.9119090437889099\n",
      "Mean Train Loss: 0.918487\n",
      "\n",
      "Epoch: 69\n",
      "Iter: 0 \tLoss: 0.8721373081207275\n",
      "Iter: 10 \tLoss: 0.8610033988952637\n",
      "Iter: 20 \tLoss: 0.9293038249015808\n",
      "Iter: 30 \tLoss: 0.9037230610847473\n",
      "Iter: 40 \tLoss: 0.9532765746116638\n",
      "Iter: 50 \tLoss: 0.8958678841590881\n",
      "Iter: 60 \tLoss: 0.8749909996986389\n",
      "Iter: 70 \tLoss: 0.8675681352615356\n",
      "Iter: 80 \tLoss: 0.8487535119056702\n",
      "Iter: 90 \tLoss: 0.9145535230636597\n",
      "Iter: 100 \tLoss: 0.860499918460846\n",
      "Iter: 110 \tLoss: 0.9071841835975647\n",
      "Mean Train Loss: 0.914651\n",
      "\n",
      "Epoch: 70\n",
      "Iter: 0 \tLoss: 0.9276894330978394\n",
      "Iter: 10 \tLoss: 0.9402447938919067\n",
      "Iter: 20 \tLoss: 0.9979436993598938\n",
      "Iter: 30 \tLoss: 0.8439052104949951\n",
      "Iter: 40 \tLoss: 0.9385794997215271\n",
      "Iter: 50 \tLoss: 0.9075601696968079\n",
      "Iter: 60 \tLoss: 0.9106317758560181\n",
      "Iter: 70 \tLoss: 0.9208337068557739\n",
      "Iter: 80 \tLoss: 0.8982825875282288\n",
      "Iter: 90 \tLoss: 0.8453038334846497\n",
      "Iter: 100 \tLoss: 1.0166374444961548\n",
      "Iter: 110 \tLoss: 0.9145869016647339\n",
      "Mean Train Loss: 0.917084\n",
      "\n",
      "Val Loss: 1.362709 \tAccuracy: 0.5713297872340426\n",
      "Epoch: 71\n",
      "Iter: 0 \tLoss: 0.9163500070571899\n",
      "Iter: 10 \tLoss: 0.8490086793899536\n",
      "Iter: 20 \tLoss: 0.9048435091972351\n",
      "Iter: 30 \tLoss: 0.9390326142311096\n",
      "Iter: 40 \tLoss: 0.8826273679733276\n",
      "Iter: 50 \tLoss: 0.9085485339164734\n",
      "Iter: 60 \tLoss: 0.9180926084518433\n",
      "Iter: 70 \tLoss: 0.8894901275634766\n",
      "Iter: 80 \tLoss: 0.9559300541877747\n",
      "Iter: 90 \tLoss: 0.9411144256591797\n",
      "Iter: 100 \tLoss: 0.9164795875549316\n",
      "Iter: 110 \tLoss: 0.8502631187438965\n",
      "Mean Train Loss: 0.913386\n",
      "\n",
      "Epoch: 72\n",
      "Iter: 0 \tLoss: 0.9096663594245911\n",
      "Iter: 10 \tLoss: 0.8913898468017578\n",
      "Iter: 20 \tLoss: 0.9102566242218018\n",
      "Iter: 30 \tLoss: 0.9051820635795593\n",
      "Iter: 40 \tLoss: 0.8629745244979858\n",
      "Iter: 50 \tLoss: 0.9180757403373718\n",
      "Iter: 60 \tLoss: 0.8504712581634521\n",
      "Iter: 70 \tLoss: 0.9690040349960327\n",
      "Iter: 80 \tLoss: 0.9200229644775391\n",
      "Iter: 90 \tLoss: 0.946723997592926\n",
      "Iter: 100 \tLoss: 0.9210253953933716\n",
      "Iter: 110 \tLoss: 0.8682234287261963\n",
      "Mean Train Loss: 0.913298\n",
      "\n",
      "Epoch: 73\n",
      "Iter: 0 \tLoss: 0.8817002177238464\n",
      "Iter: 10 \tLoss: 0.9553883671760559\n",
      "Iter: 20 \tLoss: 0.9134948253631592\n",
      "Iter: 30 \tLoss: 0.9472649097442627\n",
      "Iter: 40 \tLoss: 0.87762451171875\n",
      "Iter: 50 \tLoss: 0.9400399923324585\n",
      "Iter: 60 \tLoss: 0.9350160956382751\n",
      "Iter: 70 \tLoss: 0.9573385119438171\n",
      "Iter: 80 \tLoss: 0.8388524055480957\n",
      "Iter: 90 \tLoss: 0.9560313820838928\n",
      "Iter: 100 \tLoss: 0.9403541088104248\n",
      "Iter: 110 \tLoss: 0.900388240814209\n",
      "Mean Train Loss: 0.909343\n",
      "\n",
      "Epoch: 74\n",
      "Iter: 0 \tLoss: 0.9041324257850647\n",
      "Iter: 10 \tLoss: 0.8909611105918884\n",
      "Iter: 20 \tLoss: 0.876042366027832\n",
      "Iter: 30 \tLoss: 0.8899838328361511\n",
      "Iter: 40 \tLoss: 0.8120061755180359\n",
      "Iter: 50 \tLoss: 0.8828557729721069\n",
      "Iter: 60 \tLoss: 0.9230712056159973\n",
      "Iter: 70 \tLoss: 0.8955384492874146\n",
      "Iter: 80 \tLoss: 0.942753791809082\n",
      "Iter: 90 \tLoss: 0.9002692699432373\n",
      "Iter: 100 \tLoss: 0.9346618056297302\n",
      "Iter: 110 \tLoss: 0.9351315498352051\n",
      "Mean Train Loss: 0.906259\n",
      "\n",
      "Epoch: 75\n",
      "Iter: 0 \tLoss: 0.9099900126457214\n",
      "Iter: 10 \tLoss: 0.919065535068512\n",
      "Iter: 20 \tLoss: 0.9363162517547607\n",
      "Iter: 30 \tLoss: 1.0006309747695923\n",
      "Iter: 40 \tLoss: 0.8482732772827148\n",
      "Iter: 50 \tLoss: 0.9645281434059143\n",
      "Iter: 60 \tLoss: 0.8747746348381042\n",
      "Iter: 70 \tLoss: 0.949251651763916\n",
      "Iter: 80 \tLoss: 0.9187952876091003\n",
      "Iter: 90 \tLoss: 0.9827401041984558\n",
      "Iter: 100 \tLoss: 0.9545269012451172\n",
      "Iter: 110 \tLoss: 0.9131176471710205\n",
      "Mean Train Loss: 0.903847\n",
      "\n",
      "Val Loss: 1.413348 \tAccuracy: 0.559468085106383\n",
      "Epoch: 76\n",
      "Iter: 0 \tLoss: 0.9093887805938721\n",
      "Iter: 10 \tLoss: 0.9364774227142334\n",
      "Iter: 20 \tLoss: 0.8354160785675049\n",
      "Iter: 30 \tLoss: 0.8344724178314209\n",
      "Iter: 40 \tLoss: 0.875953197479248\n",
      "Iter: 50 \tLoss: 0.900297224521637\n",
      "Iter: 60 \tLoss: 0.9296556711196899\n",
      "Iter: 70 \tLoss: 0.904995858669281\n",
      "Iter: 80 \tLoss: 0.8824098706245422\n",
      "Iter: 90 \tLoss: 0.8747760057449341\n",
      "Iter: 100 \tLoss: 0.910318911075592\n",
      "Iter: 110 \tLoss: 0.8870983719825745\n",
      "Mean Train Loss: 0.907702\n",
      "\n",
      "Epoch: 77\n",
      "Iter: 0 \tLoss: 0.9019994139671326\n",
      "Iter: 10 \tLoss: 0.9531868696212769\n",
      "Iter: 20 \tLoss: 0.9403296113014221\n",
      "Iter: 30 \tLoss: 0.9167053699493408\n",
      "Iter: 40 \tLoss: 0.858725905418396\n",
      "Iter: 50 \tLoss: 0.8993014693260193\n",
      "Iter: 60 \tLoss: 0.8652613162994385\n",
      "Iter: 70 \tLoss: 0.8920270800590515\n",
      "Iter: 80 \tLoss: 0.8931595683097839\n",
      "Iter: 90 \tLoss: 0.9413111805915833\n",
      "Iter: 100 \tLoss: 0.924563467502594\n",
      "Iter: 110 \tLoss: 0.9068227410316467\n",
      "Mean Train Loss: 0.906372\n",
      "\n",
      "Epoch: 78\n",
      "Iter: 0 \tLoss: 0.8566035032272339\n",
      "Iter: 10 \tLoss: 0.876513659954071\n",
      "Iter: 20 \tLoss: 0.8934277296066284\n",
      "Iter: 30 \tLoss: 0.8618103861808777\n",
      "Iter: 40 \tLoss: 0.9281005859375\n",
      "Iter: 50 \tLoss: 0.836934506893158\n",
      "Iter: 60 \tLoss: 0.8950017094612122\n",
      "Iter: 70 \tLoss: 0.9297195076942444\n",
      "Iter: 80 \tLoss: 0.9142900705337524\n",
      "Iter: 90 \tLoss: 0.9290230870246887\n",
      "Iter: 100 \tLoss: 0.9347323179244995\n",
      "Iter: 110 \tLoss: 0.9032668471336365\n",
      "Mean Train Loss: 0.904497\n",
      "\n",
      "Epoch: 79\n",
      "Iter: 0 \tLoss: 0.9096789360046387\n",
      "Iter: 10 \tLoss: 0.915479302406311\n",
      "Iter: 20 \tLoss: 0.853416383266449\n",
      "Iter: 30 \tLoss: 0.8811416029930115\n",
      "Iter: 40 \tLoss: 0.9439302682876587\n",
      "Iter: 50 \tLoss: 0.8904477953910828\n",
      "Iter: 60 \tLoss: 0.929405152797699\n",
      "Iter: 70 \tLoss: 0.9253597855567932\n",
      "Iter: 80 \tLoss: 0.8439965844154358\n",
      "Iter: 90 \tLoss: 0.8175731301307678\n",
      "Iter: 100 \tLoss: 0.8914655447006226\n",
      "Iter: 110 \tLoss: 0.9163915514945984\n",
      "Mean Train Loss: 0.901252\n",
      "\n",
      "Val Loss: 1.387481 \tAccuracy: 0.5687234042553192\n"
     ]
    }
   ],
   "source": [
    "train(model_mlp, optimizer, loss_f, train_loader, val_loader, n_epoch, val_fre)\n",
    "validate(model_mlp, val_loader, loss_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7fd557",
   "metadata": {},
   "source": [
    "### Модель 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "cb247751",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hid_features, n_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding='same'),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, padding=0)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding='same'),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(1024, hid_features)\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hid_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "\n",
    "        x = self.flat(x)\n",
    "\n",
    "        # print(x.shape)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "5b3a2891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu3): ReLU()\n",
      "  (fc2): Linear(in_features=1024, out_features=47, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_mlp = MLP(in_features=28*28, hid_features=1024, n_classes=n_classes)\n",
    "print(model_mlp)\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_mlp.parameters(), lr=0.001)\n",
    "\n",
    "n_epoch = 80\n",
    "val_fre = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "8d85ad2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Iter: 0 \tLoss: 3.923083543777466\n",
      "Iter: 10 \tLoss: 1.937178134918213\n",
      "Iter: 20 \tLoss: 1.166095495223999\n",
      "Iter: 30 \tLoss: 0.8840010166168213\n",
      "Iter: 40 \tLoss: 0.8357285261154175\n",
      "Iter: 50 \tLoss: 0.7242832779884338\n",
      "Iter: 60 \tLoss: 0.6619705557823181\n",
      "Iter: 70 \tLoss: 0.695990800857544\n",
      "Iter: 80 \tLoss: 0.5852329134941101\n",
      "Iter: 90 \tLoss: 0.5461660623550415\n",
      "Iter: 100 \tLoss: 0.5850545167922974\n",
      "Iter: 110 \tLoss: 0.5809654593467712\n",
      "Mean Train Loss: 0.978270\n",
      "\n",
      "Val Loss: 0.483568 \tAccuracy: 0.8423404255319149\n",
      "Epoch: 1\n",
      "Iter: 0 \tLoss: 0.5077557563781738\n",
      "Iter: 10 \tLoss: 0.540515661239624\n",
      "Iter: 20 \tLoss: 0.5071378946304321\n",
      "Iter: 30 \tLoss: 0.46567171812057495\n",
      "Iter: 40 \tLoss: 0.4588078260421753\n",
      "Iter: 50 \tLoss: 0.4724721610546112\n",
      "Iter: 60 \tLoss: 0.49129700660705566\n",
      "Iter: 70 \tLoss: 0.45680147409439087\n",
      "Iter: 80 \tLoss: 0.44495144486427307\n",
      "Iter: 90 \tLoss: 0.44803866744041443\n",
      "Iter: 100 \tLoss: 0.47411346435546875\n",
      "Iter: 110 \tLoss: 0.44729435443878174\n",
      "Mean Train Loss: 0.475769\n",
      "\n",
      "Epoch: 2\n",
      "Iter: 0 \tLoss: 0.36874091625213623\n",
      "Iter: 10 \tLoss: 0.47323864698410034\n",
      "Iter: 20 \tLoss: 0.40251341462135315\n",
      "Iter: 30 \tLoss: 0.4289338290691376\n",
      "Iter: 40 \tLoss: 0.4477250576019287\n",
      "Iter: 50 \tLoss: 0.377964586019516\n",
      "Iter: 60 \tLoss: 0.41959795355796814\n",
      "Iter: 70 \tLoss: 0.3982847034931183\n",
      "Iter: 80 \tLoss: 0.3861783444881439\n",
      "Iter: 90 \tLoss: 0.44695237278938293\n",
      "Iter: 100 \tLoss: 0.3871425986289978\n",
      "Iter: 110 \tLoss: 0.38636884093284607\n",
      "Mean Train Loss: 0.411595\n",
      "\n",
      "Epoch: 3\n",
      "Iter: 0 \tLoss: 0.34728100895881653\n",
      "Iter: 10 \tLoss: 0.37333613634109497\n",
      "Iter: 20 \tLoss: 0.4271765947341919\n",
      "Iter: 30 \tLoss: 0.3819020688533783\n",
      "Iter: 40 \tLoss: 0.38149774074554443\n",
      "Iter: 50 \tLoss: 0.3565201461315155\n",
      "Iter: 60 \tLoss: 0.35151153802871704\n",
      "Iter: 70 \tLoss: 0.3327500522136688\n",
      "Iter: 80 \tLoss: 0.36551007628440857\n",
      "Iter: 90 \tLoss: 0.3749568462371826\n",
      "Iter: 100 \tLoss: 0.3691931664943695\n",
      "Iter: 110 \tLoss: 0.362522691488266\n",
      "Mean Train Loss: 0.375817\n",
      "\n",
      "Epoch: 4\n",
      "Iter: 0 \tLoss: 0.35720112919807434\n",
      "Iter: 10 \tLoss: 0.34616154432296753\n",
      "Iter: 20 \tLoss: 0.3680281937122345\n",
      "Iter: 30 \tLoss: 0.38071176409721375\n",
      "Iter: 40 \tLoss: 0.3209446668624878\n",
      "Iter: 50 \tLoss: 0.36893609166145325\n",
      "Iter: 60 \tLoss: 0.3277944028377533\n",
      "Iter: 70 \tLoss: 0.3758649230003357\n",
      "Iter: 80 \tLoss: 0.3281311094760895\n",
      "Iter: 90 \tLoss: 0.40246084332466125\n",
      "Iter: 100 \tLoss: 0.38229379057884216\n",
      "Iter: 110 \tLoss: 0.3540911376476288\n",
      "Mean Train Loss: 0.353443\n",
      "\n",
      "Epoch: 5\n",
      "Iter: 0 \tLoss: 0.3602704107761383\n",
      "Iter: 10 \tLoss: 0.296596884727478\n",
      "Iter: 20 \tLoss: 0.3103530704975128\n",
      "Iter: 30 \tLoss: 0.2967677712440491\n",
      "Iter: 40 \tLoss: 0.36524245142936707\n",
      "Iter: 50 \tLoss: 0.35893914103507996\n",
      "Iter: 60 \tLoss: 0.33391135931015015\n",
      "Iter: 70 \tLoss: 0.3895607888698578\n",
      "Iter: 80 \tLoss: 0.3868038058280945\n",
      "Iter: 90 \tLoss: 0.3157561719417572\n",
      "Iter: 100 \tLoss: 0.32169508934020996\n",
      "Iter: 110 \tLoss: 0.3429880440235138\n",
      "Mean Train Loss: 0.337612\n",
      "\n",
      "Val Loss: 0.343446 \tAccuracy: 0.8774468085106383\n",
      "Epoch: 6\n",
      "Iter: 0 \tLoss: 0.29605063796043396\n",
      "Iter: 10 \tLoss: 0.32476523518562317\n",
      "Iter: 20 \tLoss: 0.3174293041229248\n",
      "Iter: 30 \tLoss: 0.2880920171737671\n",
      "Iter: 40 \tLoss: 0.3024735748767853\n",
      "Iter: 50 \tLoss: 0.33319395780563354\n",
      "Iter: 60 \tLoss: 0.2847294807434082\n",
      "Iter: 70 \tLoss: 0.30780214071273804\n",
      "Iter: 80 \tLoss: 0.3502194285392761\n",
      "Iter: 90 \tLoss: 0.3402521014213562\n",
      "Iter: 100 \tLoss: 0.3588460087776184\n",
      "Iter: 110 \tLoss: 0.3567676842212677\n",
      "Mean Train Loss: 0.325180\n",
      "\n",
      "Epoch: 7\n",
      "Iter: 0 \tLoss: 0.3126223683357239\n",
      "Iter: 10 \tLoss: 0.3003063499927521\n",
      "Iter: 20 \tLoss: 0.30302664637565613\n",
      "Iter: 30 \tLoss: 0.3274643123149872\n",
      "Iter: 40 \tLoss: 0.31113213300704956\n",
      "Iter: 50 \tLoss: 0.3129792809486389\n",
      "Iter: 60 \tLoss: 0.27426373958587646\n",
      "Iter: 70 \tLoss: 0.33179211616516113\n",
      "Iter: 80 \tLoss: 0.3038882613182068\n",
      "Iter: 90 \tLoss: 0.26297780871391296\n",
      "Iter: 100 \tLoss: 0.2777693569660187\n",
      "Iter: 110 \tLoss: 0.2900893986225128\n",
      "Mean Train Loss: 0.309367\n",
      "\n",
      "Epoch: 8\n",
      "Iter: 0 \tLoss: 0.2733181118965149\n",
      "Iter: 10 \tLoss: 0.3373032808303833\n",
      "Iter: 20 \tLoss: 0.28527331352233887\n",
      "Iter: 30 \tLoss: 0.299582839012146\n",
      "Iter: 40 \tLoss: 0.3030376136302948\n",
      "Iter: 50 \tLoss: 0.31780412793159485\n",
      "Iter: 60 \tLoss: 0.2578882575035095\n",
      "Iter: 70 \tLoss: 0.33682236075401306\n",
      "Iter: 80 \tLoss: 0.28131213784217834\n",
      "Iter: 90 \tLoss: 0.28957921266555786\n",
      "Iter: 100 \tLoss: 0.30846554040908813\n",
      "Iter: 110 \tLoss: 0.30145588517189026\n",
      "Mean Train Loss: 0.297918\n",
      "\n",
      "Epoch: 9\n",
      "Iter: 0 \tLoss: 0.26830020546913147\n",
      "Iter: 10 \tLoss: 0.30962681770324707\n",
      "Iter: 20 \tLoss: 0.25616565346717834\n",
      "Iter: 30 \tLoss: 0.26762911677360535\n",
      "Iter: 40 \tLoss: 0.2892131209373474\n",
      "Iter: 50 \tLoss: 0.33098480105400085\n",
      "Iter: 60 \tLoss: 0.24515321850776672\n",
      "Iter: 70 \tLoss: 0.29176047444343567\n",
      "Iter: 80 \tLoss: 0.29125767946243286\n",
      "Iter: 90 \tLoss: 0.30495360493659973\n",
      "Iter: 100 \tLoss: 0.30655309557914734\n",
      "Iter: 110 \tLoss: 0.30292266607284546\n",
      "Mean Train Loss: 0.288201\n",
      "\n",
      "Epoch: 10\n",
      "Iter: 0 \tLoss: 0.31122931838035583\n",
      "Iter: 10 \tLoss: 0.24563804268836975\n",
      "Iter: 20 \tLoss: 0.26369357109069824\n",
      "Iter: 30 \tLoss: 0.25851061940193176\n",
      "Iter: 40 \tLoss: 0.25272995233535767\n",
      "Iter: 50 \tLoss: 0.2849974036216736\n",
      "Iter: 60 \tLoss: 0.30208656191825867\n",
      "Iter: 70 \tLoss: 0.3039510250091553\n",
      "Iter: 80 \tLoss: 0.2594352662563324\n",
      "Iter: 90 \tLoss: 0.2656855881214142\n",
      "Iter: 100 \tLoss: 0.2756709158420563\n",
      "Iter: 110 \tLoss: 0.2937055826187134\n",
      "Mean Train Loss: 0.277326\n",
      "\n",
      "Val Loss: 0.325429 \tAccuracy: 0.8847340425531914\n",
      "Val Loss: 0.325429 \tAccuracy: 0.8847340425531914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8847340425531914"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model_mlp, optimizer, loss_f, train_loader, val_loader, n_epoch, val_fre, stop_score=0.88)\n",
    "validate(model_mlp, val_loader, loss_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "80e19b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.325429 \tAccuracy: 0.8847340425531914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8847340425531914"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(model_mlp, val_loader, loss_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "430d44e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = os.path.join('myapp', 'model.ckpt')\n",
    "\n",
    "torch.save({\n",
    "            'model_state_dict': model_mlp.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'model_kwargs' : {\n",
    "                    'in_features': 28*28,\n",
    "                    'hid_features': 1024,\n",
    "                    'n_classes': n_classes,\n",
    "                }\n",
    "            }, CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "9caaaeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.325429 \tAccuracy: 0.8847340425531914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8847340425531914"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(CHECKPOINT_PATH, weights_only=True)\n",
    "\n",
    "model = MLP(**checkpoint['model_kwargs'])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "validate(model, val_loader, loss_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d40339",
   "metadata": {},
   "source": [
    "### Тест uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "11998b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>code</th>\n",
       "      <td>48</td>\n",
       "      <td>49</td>\n",
       "      <td>50</td>\n",
       "      <td>51</td>\n",
       "      <td>52</td>\n",
       "      <td>53</td>\n",
       "      <td>54</td>\n",
       "      <td>55</td>\n",
       "      <td>56</td>\n",
       "      <td>57</td>\n",
       "      <td>65</td>\n",
       "      <td>66</td>\n",
       "      <td>67</td>\n",
       "      <td>68</td>\n",
       "      <td>69</td>\n",
       "      <td>70</td>\n",
       "      <td>71</td>\n",
       "      <td>72</td>\n",
       "      <td>73</td>\n",
       "      <td>74</td>\n",
       "      <td>75</td>\n",
       "      <td>76</td>\n",
       "      <td>77</td>\n",
       "      <td>78</td>\n",
       "      <td>79</td>\n",
       "      <td>80</td>\n",
       "      <td>81</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>84</td>\n",
       "      <td>85</td>\n",
       "      <td>86</td>\n",
       "      <td>87</td>\n",
       "      <td>88</td>\n",
       "      <td>89</td>\n",
       "      <td>90</td>\n",
       "      <td>97</td>\n",
       "      <td>98</td>\n",
       "      <td>100</td>\n",
       "      <td>101</td>\n",
       "      <td>102</td>\n",
       "      <td>103</td>\n",
       "      <td>104</td>\n",
       "      <td>110</td>\n",
       "      <td>113</td>\n",
       "      <td>114</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>D</td>\n",
       "      <td>E</td>\n",
       "      <td>F</td>\n",
       "      <td>G</td>\n",
       "      <td>H</td>\n",
       "      <td>I</td>\n",
       "      <td>J</td>\n",
       "      <td>K</td>\n",
       "      <td>L</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>O</td>\n",
       "      <td>P</td>\n",
       "      <td>Q</td>\n",
       "      <td>R</td>\n",
       "      <td>S</td>\n",
       "      <td>T</td>\n",
       "      <td>U</td>\n",
       "      <td>V</td>\n",
       "      <td>W</td>\n",
       "      <td>X</td>\n",
       "      <td>Y</td>\n",
       "      <td>Z</td>\n",
       "      <td>a</td>\n",
       "      <td>b</td>\n",
       "      <td>d</td>\n",
       "      <td>e</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>h</td>\n",
       "      <td>n</td>\n",
       "      <td>q</td>\n",
       "      <td>r</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "label  0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  \\\n",
       "code   48  49  50  51  52  53  54  55  56  57  65  66  67  68  69  70  71  72   \n",
       "char    0   1   2   3   4   5   6   7   8   9   A   B   C   D   E   F   G   H   \n",
       "\n",
       "label  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  \\\n",
       "code   73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90   \n",
       "char    I   J   K   L   M   N   O   P   Q   R   S   T   U   V   W   X   Y   Z   \n",
       "\n",
       "label  36  37   38   39   40   41   42   43   44   45   46  \n",
       "code   97  98  100  101  102  103  104  110  113  114  116  \n",
       "char    a   b    d    e    f    g    h    n    q    r    t  "
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = pd.read_csv('./emnist-balanced-mapping.txt', sep=' ', names=['label', 'code'])\n",
    "mapping['char'] = mapping['code'].apply(chr)\n",
    "mapping.set_index('label', inplace=True)\n",
    "pd.set_option('display.max_columns', None)\n",
    "mapping.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "64df271c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lab2chr(label):\n",
    "    return mapping.loc[label]['char']\n",
    "\n",
    "lab2chr(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "ebf34402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch==2.4.1\n",
      "torchvision==0.19.1\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "42732a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1+cu121'"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "01d2435c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.4.1+cu121\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m261544\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:8000\u001b[0m (Press CTRL+C to quit)\n",
      "[W1023 12:49:16.616941322 NNPACK.cpp:61] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:42258 - \"\u001b[1mPOST /api/predict HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:42258 - \"\u001b[1mPOST /api/predict HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:42258 - \"\u001b[1mPOST /api/predict HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "^C\n",
      "\u001b[32mINFO\u001b[0m:     Shutting down\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
      "\u001b[32mINFO\u001b[0m:     Application shutdown complete.\n",
      "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m261544\u001b[0m]\n"
     ]
    }
   ],
   "source": [
    "!uvicorn myapp.main:app"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image-3.png": {
     "image/png": "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"
    },
    "image-4.png": {
     "image/png": "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"
    },
    "image-5.png": {
     "image/png": "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"
    },
    "image-6.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "1eea4744",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png) ![image-2.png](attachment:image-2.png) ![image-3.png](attachment:image-3.png) ![image-4.png](attachment:image-4.png) ![image-5.png](attachment:image-5.png)\n",
    "![image-6.png](attachment:image-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fe834a",
   "metadata": {},
   "source": [
    "### Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7776e40",
   "metadata": {},
   "source": [
    "`docker build -t numbers_nn .`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8199af0a",
   "metadata": {},
   "source": [
    "![alt text](WindowsTerminal_cx7VkcfVY6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be31dcfd",
   "metadata": {},
   "source": [
    "`docker run -p 8000:8000 numbers_nn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b309df",
   "metadata": {},
   "source": [
    "![alt text](WindowsTerminal_y2TIddKExW.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e894ed91",
   "metadata": {},
   "source": [
    "![alt text](brave_c3Bj4isfF7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e6c3f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
